{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACP-500.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrmp50kQNie2oZxTVm1bbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrzResearchArena/Anticancer-Peptides/blob/master/ACP-500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfsy3DVnsk_T",
        "colab_type": "code",
        "outputId": "cc35d92c-a6bc-44e3-993a-fc003c9e8012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Initialize TF-2.x:\n",
        "try:\n",
        "    %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print('We\\'re using TF-{}.'.format(tf.__version__))\n",
        "\n",
        "# Colab Auto Click: https://paste.ubuntu.com/p/R78fGBJbjb/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "We're using TF-2.1.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4XzFtwmk-SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Neural Networks:\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, BatchNormalization,\n",
        "                                     Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,\n",
        "                                     SimpleRNN, LSTM, GRU, Bidirectional, Embedding)\n",
        "from tensorflow.keras.regularizers import (l1, l2, l1_l2)\n",
        "from tensorflow.keras.optimizers import (RMSprop, Adam, SGD)\n",
        "from tensorflow.keras.models import (Sequential, Model)\n",
        "\n",
        "# Core:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Performance:\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, mean_squared_error)\n",
        "\n",
        "#Utilities\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical as labelEncoding # Usages: Ytrain = labelEncoding(Ytrain, dtype=int)\n",
        "from tensorflow.keras.utils import plot_model                      # Usages: plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False, expand_nested=True)\n",
        "from sklearn.model_selection import (StratifiedKFold, KFold, train_test_split)\n",
        "#end-import"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mM6QulVsuTP",
        "colab_type": "code",
        "outputId": "9f51890d-006a-4b13-d64c-be94bfabd913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SSddrI_x-QG",
        "colab_type": "code",
        "outputId": "c6b29b77-4755-4bd3-9dfd-5535dbfb658c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACP164.npy  acp240.npy\tACP500.npy  acp740.npy\tmodel.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqNrEhLltr4A",
        "colab_type": "code",
        "outputId": "303ccc26-7a5a-488f-83ae-c68305cba181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "cd \"drive/My Drive/ACP\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/ACP'\n",
            "/content/drive/My Drive/ACP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43uZkIQtz9A",
        "colab_type": "code",
        "outputId": "4cf2b3a3-8c7a-4d5f-e4b0-937edb0c527b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACP164.npy  acp240.npy\tACP500.npy  acp740.npy\tmodel.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_zrl45St1w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain = np.load('ACP500.npy')\n",
        "Xtest  = np.load('ACP164.npy')\n",
        "\n",
        "# ACP {500, 164}\n",
        "Ytrain  = [0 for _ in range(250)]\n",
        "Ytrain += [1 for _ in range(250)]\n",
        "Ytrain  = np.array(Ytrain)\n",
        "\n",
        "Ytest  = [0 for _ in range(82)]\n",
        "Ytest += [1 for _ in range(82)]\n",
        "\n",
        "# Ytrain  = [1 for _ in range(376)]\n",
        "# Ytrain += [0 for _ in range(364)]\n",
        "# Ytrain  = np.array(Ytrain)\n",
        "\n",
        "# Ytest  = [1 for _ in range(129)]\n",
        "# Ytest += [0 for _ in range(111)]\n",
        "\n",
        "\n",
        "Ytest  = np.array(Ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD1QPbx0uD0U",
        "colab_type": "code",
        "outputId": "15f5303a-efa3-4c4d-9bdf-2835802d205e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(Xtrain.shape)\n",
        "print(Xtest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 220)\n",
            "(164, 220)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp0_4oaHuKNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "Xtrain, Ytrain = shuffle(Xtrain, Ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgTpzib_udb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Ytrain = labelEncoding(Ytrain, dtype=int)\n",
        "Ytest  = labelEncoding(Ytest, dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEWrtbYuutyz",
        "colab_type": "code",
        "outputId": "ebcbe445-9936-43a1-e457-67bfa15bd44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Ytrain.shape\n",
        "Ytest.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(164, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdEASGGEl072",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Xtrain = np.reshape(Xtrain, (len(Xtrain), 11, 20))\n",
        "# # Xtrain.shape\n",
        "# # help(np.reshape)\n",
        "\n",
        "# # Xtrain.reshape(1,220)\n",
        "# Xtrain.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-qP_z1en51p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = 220\n",
        "D = 1\n",
        "\n",
        "Xtrain = Xtrain.reshape(500, T, D)\n",
        "Xtest  = Xtest.reshape(164,  T, D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhCpC_MJuxvw",
        "colab_type": "code",
        "outputId": "ab33150d-5016-4e6b-be60-1bf608f35162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# N, D = Xtrain.shape\n",
        "# print(N, D)\n",
        "\n",
        "# model = Sequential([\n",
        "#     Input(shape=(D)),\n",
        "#     #Dense(units=64, activation='relu'),\n",
        "#     #Dropout(0.4),\n",
        "#     Dense(units=64, activation='softmax'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(units=2, activation='softmax'),                             \n",
        "# ])\n",
        "\n",
        "# model = Sequential([\n",
        "#     LSTM(units=128, return_sequences=False, input_shape=(1, D)),\n",
        "#     Dropout(0.50),\n",
        "#     Dense(units=2, activation='softmax'),                             \n",
        "# ])\n",
        "\n",
        "\n",
        "# Inspired from VGG-16:\n",
        "# def Network():\n",
        "#     i = Input(shape=(220,1))\n",
        "#     x = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(i)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "#     x = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "#     x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=512, activation='relu')(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=256, activation='relu')(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=128, activation='relu')(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=64, activation='relu')(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=32, activation='relu')(x)\n",
        "#     x = Dropout(0.20)(x)\n",
        "#     x = Dense(units=2, activation='softmax')(x)\n",
        "#     return Model(inputs=[i], outputs=[x])\n",
        "# #end-def\n",
        "\n",
        "# Accuracy: 84.76%\n",
        "# def Network():\n",
        "#     i = Input(shape=(220,1))\n",
        "#     x = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(i)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "#     x = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     print(x.shape)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dense(units=128, activation='relu')(x)\n",
        "#     x = Dropout(0.50)(x)\n",
        "#     x = Dense(units=64, activation='relu')(x)\n",
        "#     x = Dropout(0.40)(x)\n",
        "#     x = Dense(units=32, activation='relu')(x)\n",
        "#     x = Dropout(0.30)(x)\n",
        "#     x = Dense(units=2, activation='softmax')(x)\n",
        "#     return Model(inputs=[i], outputs=[x])\n",
        "# #end-def\n",
        "\n",
        "# Accuracy: 0.8902 (Train: 0.9360) & epoch=150.\n",
        "def Network():\n",
        "    i = Input(shape=(T, D))\n",
        "\n",
        "    x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(i)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "    x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "    x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    x = Conv1D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "    x = Conv1D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    \n",
        "    x = Dense(units=128, activation='relu')(x)\n",
        "    x = Dropout(0.70)(x)\n",
        "    \n",
        "    x = Dense(units=64, activation='relu')(x)\n",
        "    x = Dropout(0.60)(x)\n",
        "\n",
        "    x = Dense(units=32, activation='relu')(x)\n",
        "    x = Dropout(0.50)(x)\n",
        "\n",
        "    x = Dense(units=16, activation='relu')(x)\n",
        "    x = Dropout(0.50)(x)\n",
        "\n",
        "    # x = Dense(units=16, activation='relu')(x)\n",
        "    # x = Dropout(0.30)(x)\n",
        "\n",
        "    x = Dense(units=2, activation='softmax')(x)\n",
        "    return Model(inputs=[i], outputs=[x])\n",
        "#end-def\n",
        "\n",
        "# def Network():\n",
        "#     i = Input(shape=(T, D))\n",
        "\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(i)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "#     x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "\n",
        "#     x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "#     x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "\n",
        "#     x = Conv1D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "#     x = Conv1D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "#     x = BatchNormalization(axis=1)(x)\n",
        "#     # print(x.shape)\n",
        "\n",
        "#     x = Flatten()(x)\n",
        "    \n",
        "#     x = Dense(units=128, activation='relu')(x)\n",
        "#     x = Dropout(0.60)(x)\n",
        "    \n",
        "#     x = Dense(units=64, activation='relu')(x)\n",
        "#     x = Dropout(0.60)(x)\n",
        "\n",
        "#     x = Dense(units=32, activation='relu')(x)\n",
        "#     x = Dropout(0.50)(x)\n",
        "\n",
        "#     x = Dense(units=16, activation='relu')(x)\n",
        "#     x = Dropout(0.50)(x)\n",
        "\n",
        "#     x = Dense(units=2, activation='softmax')(x)\n",
        "#     return Model(inputs=[i], outputs=[x])\n",
        "# #end-def\n",
        "\n",
        "\n",
        "model = Network()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 220, 64)\n",
            "(None, 220, 64)\n",
            "(None, 220, 32)\n",
            "(None, 220, 32)\n",
            "(None, 220, 16)\n",
            "(None, 220, 16)\n",
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        [(None, 220, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv1d_102 (Conv1D)          (None, 220, 64)           256       \n",
            "_________________________________________________________________\n",
            "batch_normalization_102 (Bat (None, 220, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_103 (Conv1D)          (None, 220, 64)           12352     \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 220, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_104 (Conv1D)          (None, 220, 32)           6176      \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 220, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_105 (Conv1D)          (None, 220, 32)           3104      \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 220, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_106 (Conv1D)          (None, 220, 16)           1552      \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 220, 16)           64        \n",
            "_________________________________________________________________\n",
            "conv1d_107 (Conv1D)          (None, 220, 16)           784       \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 220, 16)           64        \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 3520)              0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 128)               450688    \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 486,706\n",
            "Trainable params: 486,258\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLq_OrYwMPkG",
        "colab_type": "code",
        "outputId": "d083251e-4c06-432b-a5a7-f79996622832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 10-fold Cross-Validation\n",
        "\n",
        "# print(cv)\n",
        "\n",
        "# N = 100\n",
        "# C = 1\n",
        "\n",
        "\n",
        "# cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# Train_index = []\n",
        "# Test_index = []\n",
        "# for (train_index, test_index) in cv.split(Xtrain, Ytrain.argmax(axis=1)):\n",
        "#     Train_index.append(train_index)\n",
        "#     Test_index.append(test_index)\n",
        "# #end-for\n",
        "\n",
        "\n",
        "# Accuracy = []\n",
        "# kfold = KFold(n_splits=5,shuffle=False)\n",
        "\n",
        "# for train_index, test_index in kfold.split(Xtrain):\n",
        "#     model = Network()\n",
        "#     model.compile(optimizer=Adam(),\n",
        "#                   loss='categorical_crossentropy',\n",
        "#                   metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     # model.fit(x=[Xtrain[train_index,:,:]], y=[Ytrain[train_index,:]], validation_data=([Xtrain[test_index,:,:], Ytrain[test_index,:]]), epochs=500, verbose=2)\n",
        "#     model.fit(Xtrain[train_index,:,:], Ytrain[train_index,:], epochs=100, verbose=2)\n",
        "#     accuracy = model.evaluate(Xtrain[test_index,:,:], Ytrain[test_index,:], verbose=0)\n",
        "#     Accuracy.append(accuracy)\n",
        "#     print('Fold {}, accuracy: {}'.format(C, acc))\n",
        "\n",
        "# print(Accuracy)\n",
        "# np.mean(Accuracy)\n",
        "\n",
        "model = Network()\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "# result = model.fit(x=Xtrain, y=Ytrain, validation_split=0.20, epochs=200, verbose=1)\n",
        "result = model.fit(x=Xtrain, y=Ytrain, validation_data=(Xtest, Ytest), epochs=120, verbose=1)\n",
        "\n",
        "# accuracy = model.evaluate(Xtrain[, Ytrain[:,:], verbose=0)\n",
        "# Accuracy.append(accuracy)\n",
        "# print('Fold {}, accuracy: {}'.format(C, acc))\n",
        "\n",
        "# print(Accuracy)\n",
        "# np.mean(Accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 220, 64)\n",
            "(None, 220, 64)\n",
            "(None, 220, 32)\n",
            "(None, 220, 32)\n",
            "(None, 220, 16)\n",
            "(None, 220, 16)\n",
            "Train on 500 samples, validate on 164 samples\n",
            "Epoch 1/120\n",
            "500/500 [==============================] - 2s 4ms/sample - loss: 1.7459 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.6159\n",
            "Epoch 2/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 1.5381 - accuracy: 0.4840 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
            "Epoch 3/120\n",
            "500/500 [==============================] - 0s 346us/sample - loss: 1.1073 - accuracy: 0.4900 - val_loss: 0.6924 - val_accuracy: 0.5122\n",
            "Epoch 4/120\n",
            "500/500 [==============================] - 0s 373us/sample - loss: 0.8910 - accuracy: 0.5620 - val_loss: 0.6920 - val_accuracy: 0.5732\n",
            "Epoch 5/120\n",
            "500/500 [==============================] - 0s 372us/sample - loss: 0.8992 - accuracy: 0.5240 - val_loss: 0.6923 - val_accuracy: 0.5122\n",
            "Epoch 6/120\n",
            "500/500 [==============================] - 0s 386us/sample - loss: 0.8307 - accuracy: 0.5200 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
            "Epoch 7/120\n",
            "500/500 [==============================] - 0s 350us/sample - loss: 0.8196 - accuracy: 0.5200 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 8/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 0.7698 - accuracy: 0.5320 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
            "Epoch 9/120\n",
            "500/500 [==============================] - 0s 385us/sample - loss: 0.7806 - accuracy: 0.5420 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 10/120\n",
            "500/500 [==============================] - 0s 359us/sample - loss: 0.8150 - accuracy: 0.5040 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 11/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.7844 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 12/120\n",
            "500/500 [==============================] - 0s 364us/sample - loss: 0.6997 - accuracy: 0.5400 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
            "Epoch 13/120\n",
            "500/500 [==============================] - 0s 370us/sample - loss: 0.7218 - accuracy: 0.4940 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 14/120\n",
            "500/500 [==============================] - 0s 346us/sample - loss: 0.6923 - accuracy: 0.5340 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
            "Epoch 15/120\n",
            "500/500 [==============================] - 0s 388us/sample - loss: 0.7138 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5183\n",
            "Epoch 16/120\n",
            "500/500 [==============================] - 0s 372us/sample - loss: 0.7378 - accuracy: 0.5240 - val_loss: 0.6916 - val_accuracy: 0.5366\n",
            "Epoch 17/120\n",
            "500/500 [==============================] - 0s 386us/sample - loss: 0.6950 - accuracy: 0.5380 - val_loss: 0.6900 - val_accuracy: 0.5854\n",
            "Epoch 18/120\n",
            "500/500 [==============================] - 0s 363us/sample - loss: 0.7169 - accuracy: 0.5080 - val_loss: 0.6890 - val_accuracy: 0.6220\n",
            "Epoch 19/120\n",
            "500/500 [==============================] - 0s 384us/sample - loss: 0.6937 - accuracy: 0.5420 - val_loss: 0.6881 - val_accuracy: 0.6159\n",
            "Epoch 20/120\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.7208 - accuracy: 0.5480 - val_loss: 0.6864 - val_accuracy: 0.6768\n",
            "Epoch 21/120\n",
            "500/500 [==============================] - 0s 367us/sample - loss: 0.7023 - accuracy: 0.5580 - val_loss: 0.6886 - val_accuracy: 0.6341\n",
            "Epoch 22/120\n",
            "500/500 [==============================] - 0s 367us/sample - loss: 0.6856 - accuracy: 0.5820 - val_loss: 0.6871 - val_accuracy: 0.6463\n",
            "Epoch 23/120\n",
            "500/500 [==============================] - 0s 365us/sample - loss: 0.6705 - accuracy: 0.5980 - val_loss: 0.6817 - val_accuracy: 0.6951\n",
            "Epoch 24/120\n",
            "500/500 [==============================] - 0s 352us/sample - loss: 0.6796 - accuracy: 0.5760 - val_loss: 0.6847 - val_accuracy: 0.6463\n",
            "Epoch 25/120\n",
            "500/500 [==============================] - 0s 390us/sample - loss: 0.6729 - accuracy: 0.5840 - val_loss: 0.6823 - val_accuracy: 0.6707\n",
            "Epoch 26/120\n",
            "500/500 [==============================] - 0s 383us/sample - loss: 0.6707 - accuracy: 0.5940 - val_loss: 0.6796 - val_accuracy: 0.7073\n",
            "Epoch 27/120\n",
            "500/500 [==============================] - 0s 357us/sample - loss: 0.6597 - accuracy: 0.6260 - val_loss: 0.6801 - val_accuracy: 0.7134\n",
            "Epoch 28/120\n",
            "500/500 [==============================] - 0s 351us/sample - loss: 0.6840 - accuracy: 0.5920 - val_loss: 0.6692 - val_accuracy: 0.7500\n",
            "Epoch 29/120\n",
            "500/500 [==============================] - 0s 347us/sample - loss: 0.6576 - accuracy: 0.5860 - val_loss: 0.6539 - val_accuracy: 0.7561\n",
            "Epoch 30/120\n",
            "500/500 [==============================] - 0s 353us/sample - loss: 0.6530 - accuracy: 0.6220 - val_loss: 0.6548 - val_accuracy: 0.7561\n",
            "Epoch 31/120\n",
            "500/500 [==============================] - 0s 371us/sample - loss: 0.6682 - accuracy: 0.6440 - val_loss: 0.6539 - val_accuracy: 0.7805\n",
            "Epoch 32/120\n",
            "500/500 [==============================] - 0s 366us/sample - loss: 0.6635 - accuracy: 0.6300 - val_loss: 0.6453 - val_accuracy: 0.7988\n",
            "Epoch 33/120\n",
            "500/500 [==============================] - 0s 343us/sample - loss: 0.6308 - accuracy: 0.6460 - val_loss: 0.6423 - val_accuracy: 0.7622\n",
            "Epoch 34/120\n",
            "500/500 [==============================] - 0s 356us/sample - loss: 0.6408 - accuracy: 0.6380 - val_loss: 0.6354 - val_accuracy: 0.7744\n",
            "Epoch 35/120\n",
            "500/500 [==============================] - 0s 377us/sample - loss: 0.6207 - accuracy: 0.6820 - val_loss: 0.6280 - val_accuracy: 0.7927\n",
            "Epoch 36/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.6275 - accuracy: 0.6680 - val_loss: 0.6177 - val_accuracy: 0.8049\n",
            "Epoch 37/120\n",
            "500/500 [==============================] - 0s 364us/sample - loss: 0.6284 - accuracy: 0.6540 - val_loss: 0.6085 - val_accuracy: 0.7927\n",
            "Epoch 38/120\n",
            "500/500 [==============================] - 0s 343us/sample - loss: 0.5972 - accuracy: 0.6700 - val_loss: 0.5833 - val_accuracy: 0.8171\n",
            "Epoch 39/120\n",
            "500/500 [==============================] - 0s 346us/sample - loss: 0.5930 - accuracy: 0.6920 - val_loss: 0.5649 - val_accuracy: 0.8049\n",
            "Epoch 40/120\n",
            "500/500 [==============================] - 0s 348us/sample - loss: 0.5634 - accuracy: 0.7200 - val_loss: 0.5493 - val_accuracy: 0.8110\n",
            "Epoch 41/120\n",
            "500/500 [==============================] - 0s 325us/sample - loss: 0.5673 - accuracy: 0.7300 - val_loss: 0.5330 - val_accuracy: 0.8232\n",
            "Epoch 42/120\n",
            "500/500 [==============================] - 0s 366us/sample - loss: 0.5486 - accuracy: 0.7320 - val_loss: 0.4920 - val_accuracy: 0.8415\n",
            "Epoch 43/120\n",
            "500/500 [==============================] - 0s 367us/sample - loss: 0.5666 - accuracy: 0.7560 - val_loss: 0.5132 - val_accuracy: 0.8171\n",
            "Epoch 44/120\n",
            "500/500 [==============================] - 0s 374us/sample - loss: 0.5592 - accuracy: 0.7800 - val_loss: 0.5004 - val_accuracy: 0.8293\n",
            "Epoch 45/120\n",
            "500/500 [==============================] - 0s 353us/sample - loss: 0.5324 - accuracy: 0.7620 - val_loss: 0.4944 - val_accuracy: 0.8293\n",
            "Epoch 46/120\n",
            "500/500 [==============================] - 0s 340us/sample - loss: 0.5256 - accuracy: 0.7680 - val_loss: 0.4729 - val_accuracy: 0.8598\n",
            "Epoch 47/120\n",
            "500/500 [==============================] - 0s 360us/sample - loss: 0.5233 - accuracy: 0.7740 - val_loss: 0.4786 - val_accuracy: 0.8537\n",
            "Epoch 48/120\n",
            "500/500 [==============================] - 0s 387us/sample - loss: 0.4816 - accuracy: 0.8120 - val_loss: 0.4498 - val_accuracy: 0.8720\n",
            "Epoch 49/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.4994 - accuracy: 0.8040 - val_loss: 0.4367 - val_accuracy: 0.8659\n",
            "Epoch 50/120\n",
            "500/500 [==============================] - 0s 376us/sample - loss: 0.4788 - accuracy: 0.8060 - val_loss: 0.4464 - val_accuracy: 0.8293\n",
            "Epoch 51/120\n",
            "500/500 [==============================] - 0s 352us/sample - loss: 0.4649 - accuracy: 0.8040 - val_loss: 0.4351 - val_accuracy: 0.8476\n",
            "Epoch 52/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 0.4635 - accuracy: 0.8100 - val_loss: 0.4195 - val_accuracy: 0.8537\n",
            "Epoch 53/120\n",
            "500/500 [==============================] - 0s 362us/sample - loss: 0.4504 - accuracy: 0.8100 - val_loss: 0.4217 - val_accuracy: 0.8415\n",
            "Epoch 54/120\n",
            "500/500 [==============================] - 0s 366us/sample - loss: 0.4289 - accuracy: 0.8320 - val_loss: 0.4082 - val_accuracy: 0.8598\n",
            "Epoch 55/120\n",
            "500/500 [==============================] - 0s 358us/sample - loss: 0.4368 - accuracy: 0.8340 - val_loss: 0.4023 - val_accuracy: 0.8537\n",
            "Epoch 56/120\n",
            "500/500 [==============================] - 0s 375us/sample - loss: 0.3940 - accuracy: 0.8500 - val_loss: 0.4032 - val_accuracy: 0.8537\n",
            "Epoch 57/120\n",
            "500/500 [==============================] - 0s 364us/sample - loss: 0.3920 - accuracy: 0.8620 - val_loss: 0.4224 - val_accuracy: 0.7927\n",
            "Epoch 58/120\n",
            "500/500 [==============================] - 0s 358us/sample - loss: 0.3918 - accuracy: 0.8500 - val_loss: 0.4025 - val_accuracy: 0.8415\n",
            "Epoch 59/120\n",
            "500/500 [==============================] - 0s 351us/sample - loss: 0.4307 - accuracy: 0.8240 - val_loss: 0.3934 - val_accuracy: 0.8354\n",
            "Epoch 60/120\n",
            "500/500 [==============================] - 0s 348us/sample - loss: 0.4021 - accuracy: 0.8420 - val_loss: 0.3808 - val_accuracy: 0.8659\n",
            "Epoch 61/120\n",
            "500/500 [==============================] - 0s 344us/sample - loss: 0.4003 - accuracy: 0.8420 - val_loss: 0.4236 - val_accuracy: 0.8354\n",
            "Epoch 62/120\n",
            "500/500 [==============================] - 0s 356us/sample - loss: 0.4097 - accuracy: 0.8380 - val_loss: 0.3863 - val_accuracy: 0.8476\n",
            "Epoch 63/120\n",
            "500/500 [==============================] - 0s 332us/sample - loss: 0.4061 - accuracy: 0.8400 - val_loss: 0.3868 - val_accuracy: 0.8537\n",
            "Epoch 64/120\n",
            "500/500 [==============================] - 0s 359us/sample - loss: 0.4113 - accuracy: 0.8420 - val_loss: 0.4137 - val_accuracy: 0.7866\n",
            "Epoch 65/120\n",
            "500/500 [==============================] - 0s 373us/sample - loss: 0.3819 - accuracy: 0.8640 - val_loss: 0.3851 - val_accuracy: 0.8537\n",
            "Epoch 66/120\n",
            "500/500 [==============================] - 0s 357us/sample - loss: 0.3614 - accuracy: 0.8620 - val_loss: 0.4336 - val_accuracy: 0.8354\n",
            "Epoch 67/120\n",
            "500/500 [==============================] - 0s 342us/sample - loss: 0.3871 - accuracy: 0.8660 - val_loss: 0.3865 - val_accuracy: 0.8415\n",
            "Epoch 68/120\n",
            "500/500 [==============================] - 0s 341us/sample - loss: 0.3820 - accuracy: 0.8640 - val_loss: 0.3804 - val_accuracy: 0.8354\n",
            "Epoch 69/120\n",
            "500/500 [==============================] - 0s 330us/sample - loss: 0.3910 - accuracy: 0.8380 - val_loss: 0.3687 - val_accuracy: 0.8232\n",
            "Epoch 70/120\n",
            "500/500 [==============================] - 0s 352us/sample - loss: 0.4016 - accuracy: 0.8480 - val_loss: 0.3610 - val_accuracy: 0.8598\n",
            "Epoch 71/120\n",
            "500/500 [==============================] - 0s 387us/sample - loss: 0.3551 - accuracy: 0.8560 - val_loss: 0.3618 - val_accuracy: 0.8537\n",
            "Epoch 72/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.3695 - accuracy: 0.8720 - val_loss: 0.3794 - val_accuracy: 0.8659\n",
            "Epoch 73/120\n",
            "500/500 [==============================] - 0s 352us/sample - loss: 0.3976 - accuracy: 0.8380 - val_loss: 0.3516 - val_accuracy: 0.8598\n",
            "Epoch 74/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.3459 - accuracy: 0.8860 - val_loss: 0.3468 - val_accuracy: 0.8476\n",
            "Epoch 75/120\n",
            "500/500 [==============================] - 0s 322us/sample - loss: 0.3640 - accuracy: 0.8640 - val_loss: 0.3441 - val_accuracy: 0.8537\n",
            "Epoch 76/120\n",
            "500/500 [==============================] - 0s 342us/sample - loss: 0.3997 - accuracy: 0.8620 - val_loss: 0.3634 - val_accuracy: 0.8415\n",
            "Epoch 77/120\n",
            "500/500 [==============================] - 0s 375us/sample - loss: 0.4143 - accuracy: 0.8300 - val_loss: 0.3656 - val_accuracy: 0.8476\n",
            "Epoch 78/120\n",
            "500/500 [==============================] - 0s 351us/sample - loss: 0.3870 - accuracy: 0.8460 - val_loss: 0.3712 - val_accuracy: 0.8354\n",
            "Epoch 79/120\n",
            "500/500 [==============================] - 0s 346us/sample - loss: 0.3545 - accuracy: 0.8640 - val_loss: 0.3528 - val_accuracy: 0.8659\n",
            "Epoch 80/120\n",
            "500/500 [==============================] - 0s 329us/sample - loss: 0.3177 - accuracy: 0.8800 - val_loss: 0.3759 - val_accuracy: 0.8720\n",
            "Epoch 81/120\n",
            "500/500 [==============================] - 0s 338us/sample - loss: 0.3328 - accuracy: 0.8740 - val_loss: 0.3619 - val_accuracy: 0.8415\n",
            "Epoch 82/120\n",
            "500/500 [==============================] - 0s 348us/sample - loss: 0.3421 - accuracy: 0.8840 - val_loss: 0.3854 - val_accuracy: 0.8293\n",
            "Epoch 83/120\n",
            "500/500 [==============================] - 0s 353us/sample - loss: 0.3236 - accuracy: 0.8700 - val_loss: 0.3606 - val_accuracy: 0.8415\n",
            "Epoch 84/120\n",
            "500/500 [==============================] - 0s 371us/sample - loss: 0.3679 - accuracy: 0.8540 - val_loss: 0.3557 - val_accuracy: 0.8537\n",
            "Epoch 85/120\n",
            "500/500 [==============================] - 0s 370us/sample - loss: 0.3168 - accuracy: 0.8820 - val_loss: 0.3361 - val_accuracy: 0.8720\n",
            "Epoch 86/120\n",
            "500/500 [==============================] - 0s 365us/sample - loss: 0.3110 - accuracy: 0.8940 - val_loss: 0.3475 - val_accuracy: 0.8659\n",
            "Epoch 87/120\n",
            "500/500 [==============================] - 0s 338us/sample - loss: 0.3125 - accuracy: 0.8820 - val_loss: 0.3682 - val_accuracy: 0.8293\n",
            "Epoch 88/120\n",
            "500/500 [==============================] - 0s 380us/sample - loss: 0.3191 - accuracy: 0.8940 - val_loss: 0.3261 - val_accuracy: 0.8537\n",
            "Epoch 89/120\n",
            "500/500 [==============================] - 0s 353us/sample - loss: 0.2792 - accuracy: 0.9000 - val_loss: 0.3182 - val_accuracy: 0.8659\n",
            "Epoch 90/120\n",
            "500/500 [==============================] - 0s 342us/sample - loss: 0.2832 - accuracy: 0.8980 - val_loss: 0.3210 - val_accuracy: 0.8476\n",
            "Epoch 91/120\n",
            "500/500 [==============================] - 0s 343us/sample - loss: 0.2875 - accuracy: 0.9040 - val_loss: 0.3232 - val_accuracy: 0.8598\n",
            "Epoch 92/120\n",
            "500/500 [==============================] - 0s 380us/sample - loss: 0.2788 - accuracy: 0.9100 - val_loss: 0.3112 - val_accuracy: 0.8659\n",
            "Epoch 93/120\n",
            "500/500 [==============================] - 0s 393us/sample - loss: 0.2725 - accuracy: 0.9180 - val_loss: 0.3201 - val_accuracy: 0.8720\n",
            "Epoch 94/120\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.3185 - accuracy: 0.8960 - val_loss: 0.3174 - val_accuracy: 0.8780\n",
            "Epoch 95/120\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.2904 - accuracy: 0.8980 - val_loss: 0.3290 - val_accuracy: 0.8659\n",
            "Epoch 96/120\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.2943 - accuracy: 0.9060 - val_loss: 0.3258 - val_accuracy: 0.8598\n",
            "Epoch 97/120\n",
            "500/500 [==============================] - 0s 376us/sample - loss: 0.3215 - accuracy: 0.9040 - val_loss: 0.3494 - val_accuracy: 0.8354\n",
            "Epoch 98/120\n",
            "500/500 [==============================] - 0s 381us/sample - loss: 0.3393 - accuracy: 0.8840 - val_loss: 0.5489 - val_accuracy: 0.7866\n",
            "Epoch 99/120\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.3269 - accuracy: 0.8760 - val_loss: 0.3254 - val_accuracy: 0.8598\n",
            "Epoch 100/120\n",
            "500/500 [==============================] - 0s 371us/sample - loss: 0.2869 - accuracy: 0.8880 - val_loss: 0.3188 - val_accuracy: 0.8720\n",
            "Epoch 101/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 0.3076 - accuracy: 0.9000 - val_loss: 0.3140 - val_accuracy: 0.8720\n",
            "Epoch 102/120\n",
            "500/500 [==============================] - 0s 357us/sample - loss: 0.2713 - accuracy: 0.9040 - val_loss: 0.3061 - val_accuracy: 0.8780\n",
            "Epoch 103/120\n",
            "500/500 [==============================] - 0s 354us/sample - loss: 0.2722 - accuracy: 0.9100 - val_loss: 0.3106 - val_accuracy: 0.8841\n",
            "Epoch 104/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 0.2658 - accuracy: 0.9120 - val_loss: 0.3082 - val_accuracy: 0.8720\n",
            "Epoch 105/120\n",
            "500/500 [==============================] - 0s 351us/sample - loss: 0.2916 - accuracy: 0.8920 - val_loss: 0.3075 - val_accuracy: 0.8780\n",
            "Epoch 106/120\n",
            "500/500 [==============================] - 0s 368us/sample - loss: 0.2648 - accuracy: 0.9020 - val_loss: 0.3534 - val_accuracy: 0.8537\n",
            "Epoch 107/120\n",
            "500/500 [==============================] - 0s 377us/sample - loss: 0.2596 - accuracy: 0.9120 - val_loss: 0.3556 - val_accuracy: 0.8415\n",
            "Epoch 108/120\n",
            "500/500 [==============================] - 0s 373us/sample - loss: 0.2407 - accuracy: 0.9280 - val_loss: 0.3272 - val_accuracy: 0.8476\n",
            "Epoch 109/120\n",
            "500/500 [==============================] - 0s 374us/sample - loss: 0.2296 - accuracy: 0.9260 - val_loss: 0.3238 - val_accuracy: 0.8720\n",
            "Epoch 110/120\n",
            "500/500 [==============================] - 0s 360us/sample - loss: 0.2407 - accuracy: 0.9180 - val_loss: 0.3574 - val_accuracy: 0.8537\n",
            "Epoch 111/120\n",
            "500/500 [==============================] - 0s 360us/sample - loss: 0.2162 - accuracy: 0.9260 - val_loss: 0.3376 - val_accuracy: 0.8537\n",
            "Epoch 112/120\n",
            "500/500 [==============================] - 0s 362us/sample - loss: 0.2467 - accuracy: 0.9100 - val_loss: 0.3513 - val_accuracy: 0.8659\n",
            "Epoch 113/120\n",
            "500/500 [==============================] - 0s 373us/sample - loss: 0.2468 - accuracy: 0.9120 - val_loss: 0.4027 - val_accuracy: 0.8598\n",
            "Epoch 114/120\n",
            "500/500 [==============================] - 0s 375us/sample - loss: 0.2524 - accuracy: 0.9220 - val_loss: 0.3336 - val_accuracy: 0.8415\n",
            "Epoch 115/120\n",
            "500/500 [==============================] - 0s 361us/sample - loss: 0.2100 - accuracy: 0.9300 - val_loss: 0.3898 - val_accuracy: 0.8232\n",
            "Epoch 116/120\n",
            "500/500 [==============================] - 0s 337us/sample - loss: 0.2251 - accuracy: 0.9240 - val_loss: 0.5109 - val_accuracy: 0.8049\n",
            "Epoch 117/120\n",
            "500/500 [==============================] - 0s 355us/sample - loss: 0.2417 - accuracy: 0.9120 - val_loss: 0.4687 - val_accuracy: 0.7988\n",
            "Epoch 118/120\n",
            "500/500 [==============================] - 0s 360us/sample - loss: 0.2643 - accuracy: 0.9120 - val_loss: 0.3632 - val_accuracy: 0.8537\n",
            "Epoch 119/120\n",
            "500/500 [==============================] - 0s 365us/sample - loss: 0.2397 - accuracy: 0.9200 - val_loss: 0.4062 - val_accuracy: 0.7927\n",
            "Epoch 120/120\n",
            "500/500 [==============================] - 0s 347us/sample - loss: 0.3182 - accuracy: 0.9080 - val_loss: 0.3399 - val_accuracy: 0.8537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkl8utkLdKW1",
        "colab_type": "code",
        "outputId": "08d8b462-4899-463c-b7b6-4a47fa0a6911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# i = 0\n",
        "\n",
        "# model = Network()\n",
        "# model.compile(optimizer=Adam(),\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# model.fit(Xtrain[Train_index[i],:,:], Ytrain[Train_index[i],:], validation_data=(Xtrain[Test_index[i],:,:], Ytrain[Test_index[i],:]), epochs=500, verbose=2) #batch_size=32\n",
        "# acc = model.evaluate(Xtrain[Test_index[i],:,:], Ytrain[Test_index[i],:], verbose=0)\n",
        "# print('Fold {}, accuracy: {}'.format(C, acc))\n",
        "\n",
        "\n",
        "len(Ytrain[Train_index[5],:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-9770b23c202a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4cDNZuAVuFa",
        "colab_type": "code",
        "outputId": "a3546966-0e6e-4c85-98af-0a3652a67c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "help(model.fit)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method fit in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs) method of tensorflow.python.keras.engine.training.Model instance\n",
            "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            "    \n",
            "    Arguments:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            "            or `(inputs, targets, sample weights)`.\n",
            "          A more detailed description of unpacking behavior for iterator types\n",
            "          (Dataset, generator, Sequence) is given below.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of symbolic tensors, datasets,\n",
            "            generators, or `keras.utils.Sequence` instances (since they generate\n",
            "            batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided.\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            Note that the progress bar is not particularly useful when\n",
            "            logged to a file, so verbose=2 is recommended when not running\n",
            "            interactively (eg, in a production environment).\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This argument is\n",
            "            not supported when `x` is a dataset, generator or\n",
            "           `keras.utils.Sequence` instance.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            "              - dataset\n",
            "            For the first two cases, `batch_size` must be provided.\n",
            "            For the last case, `validation_steps` could be provided.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch').\n",
            "            'batch' is a special option for dealing with the\n",
            "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
            "            Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample.\n",
            "            In this case you should make sure to specify\n",
            "            `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
            "            supported when `x` is a dataset, generator, or\n",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            "            as the third element of `x`.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is exhausted.\n",
            "            This argument is not supported with array inputs.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None, validation\n",
            "            will run until the `validation_data` dataset is exhausted. In the\n",
            "            case of a infinite dataset, it will run into a infinite loop.\n",
            "            If 'validation_steps' is specified and only part of the dataset\n",
            "            will be consumed, the evaluation will start from the beginning of\n",
            "            the dataset at each epoch. This ensures that the same validation\n",
            "            samples are used every time.\n",
            "        validation_freq: Only relevant if validation data is provided. Integer\n",
            "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            "            If an integer, specifies how many training epochs to run before a\n",
            "            new validation run is performed, e.g. `validation_freq=2` runs\n",
            "            validation every 2 epochs. If a Container, specifies the epochs on\n",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            "            input only. Maximum size for the generator queue.\n",
            "            If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1. If 0, will execute the generator on the main\n",
            "            thread.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-picklable arguments to\n",
            "            the generator as they can't be passed easily to children processes.\n",
            "        **kwargs: Used for backwards compatibility.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample weights.\n",
            "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            "      second and third elements will be used for y and sample_weight\n",
            "      respectively. Any other type provided will be wrapped in a length one\n",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            "      should still adhere to the top-level tuple structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is that\n",
            "      it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            "      and sample_weight or passed through as a single element to `x`. As a\n",
            "      result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: If the model was never compiled.\n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYW_VUs-u-Ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xYbkNZ6vEey",
        "colab_type": "code",
        "outputId": "960e241f-2fe3-43e6-c06d-246efe81d236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "monitor = EarlyStopping(monitor='val_loss', verbose=1, patience=5, min_delta=1e-3)\n",
        "\n",
        "result = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), epochs=200) #batch_size=32"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 500 samples, validate on 164 samples\n",
            "Epoch 1/200\n",
            "500/500 [==============================] - 3s 6ms/sample - loss: 0.6938 - accuracy: 0.4560 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 2/200\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.6939 - accuracy: 0.4880 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/200\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.6928 - accuracy: 0.5140 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 4/200\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6936 - accuracy: 0.4780 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/200\n",
            "500/500 [==============================] - 0s 395us/sample - loss: 0.6931 - accuracy: 0.5220 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "500/500 [==============================] - 0s 452us/sample - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 7/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6930 - accuracy: 0.5300 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 8/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6932 - accuracy: 0.5140 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 9/200\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.6930 - accuracy: 0.5220 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 10/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6937 - accuracy: 0.4960 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 11/200\n",
            "500/500 [==============================] - 0s 491us/sample - loss: 0.6934 - accuracy: 0.4820 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 12/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 13/200\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.6931 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 14/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6937 - accuracy: 0.4700 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "500/500 [==============================] - 0s 437us/sample - loss: 0.6932 - accuracy: 0.4880 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 16/200\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.6936 - accuracy: 0.4760 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 17/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6932 - accuracy: 0.4860 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 18/200\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.6933 - accuracy: 0.4860 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 19/200\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.6935 - accuracy: 0.4840 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 20/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6929 - accuracy: 0.5160 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 21/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6933 - accuracy: 0.4840 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 22/200\n",
            "500/500 [==============================] - 0s 394us/sample - loss: 0.6934 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 23/200\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 24/200\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 25/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6931 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 26/200\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.6934 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 27/200\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6932 - accuracy: 0.5060 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 28/200\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6928 - accuracy: 0.5180 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 29/200\n",
            "500/500 [==============================] - 0s 460us/sample - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 30/200\n",
            "500/500 [==============================] - 0s 452us/sample - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 31/200\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.6924 - accuracy: 0.5340 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 32/200\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6933 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 33/200\n",
            "500/500 [==============================] - 0s 448us/sample - loss: 0.6930 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 34/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6937 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 35/200\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.6937 - accuracy: 0.4860 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 36/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6937 - accuracy: 0.4800 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 37/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6932 - accuracy: 0.5080 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 38/200\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.6929 - accuracy: 0.5160 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 39/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6929 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 40/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6931 - accuracy: 0.4860 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 41/200\n",
            "500/500 [==============================] - 0s 441us/sample - loss: 0.6937 - accuracy: 0.4900 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 42/200\n",
            "500/500 [==============================] - 0s 449us/sample - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 43/200\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6931 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 44/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6932 - accuracy: 0.5280 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 45/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6931 - accuracy: 0.5300 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 46/200\n",
            "500/500 [==============================] - 0s 463us/sample - loss: 0.6934 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 47/200\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.6937 - accuracy: 0.4680 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 48/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6928 - accuracy: 0.5160 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 49/200\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.6930 - accuracy: 0.5180 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 50/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6925 - accuracy: 0.5260 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 51/200\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.6938 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 52/200\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 53/200\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.6937 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 54/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6929 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 55/200\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.6934 - accuracy: 0.4660 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 56/200\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.6927 - accuracy: 0.5140 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 57/200\n",
            "500/500 [==============================] - 0s 463us/sample - loss: 0.6934 - accuracy: 0.4980 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 58/200\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.6939 - accuracy: 0.4620 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 59/200\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6933 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 60/200\n",
            "500/500 [==============================] - 0s 440us/sample - loss: 0.6931 - accuracy: 0.5140 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 61/200\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.6938 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 62/200\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.6925 - accuracy: 0.5320 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 63/200\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6930 - accuracy: 0.5180 - val_loss: 0.6899 - val_accuracy: 0.5244\n",
            "Epoch 64/200\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6938 - accuracy: 0.4760 - val_loss: 0.6896 - val_accuracy: 0.5366\n",
            "Epoch 65/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6928 - accuracy: 0.5240 - val_loss: 0.6914 - val_accuracy: 0.4939\n",
            "Epoch 66/200\n",
            "500/500 [==============================] - 0s 479us/sample - loss: 0.6930 - accuracy: 0.5160 - val_loss: 0.6929 - val_accuracy: 0.5244\n",
            "Epoch 67/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6928 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.5122\n",
            "Epoch 68/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6929 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.4939\n",
            "Epoch 69/200\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.6945 - accuracy: 0.4560 - val_loss: 0.6931 - val_accuracy: 0.5183\n",
            "Epoch 70/200\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.6927 - accuracy: 0.5280 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 71/200\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.6933 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 72/200\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 73/200\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 74/200\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.6939 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 75/200\n",
            "500/500 [==============================] - 0s 461us/sample - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 76/200\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.6940 - accuracy: 0.4400 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 77/200\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.6933 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 78/200\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.6931 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 79/200\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6935 - accuracy: 0.4840 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 80/200\n",
            "500/500 [==============================] - 0s 460us/sample - loss: 0.6934 - accuracy: 0.4820 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 81/200\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 82/200\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 83/200\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6930 - accuracy: 0.5140 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6936 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 85/200\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.6932 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 86/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6935 - accuracy: 0.4720 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 87/200\n",
            "500/500 [==============================] - 0s 437us/sample - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 88/200\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 89/200\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 90/200\n",
            "500/500 [==============================] - 0s 459us/sample - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 91/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 92/200\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.6932 - accuracy: 0.4960 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 93/200\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 94/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 95/200\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 96/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 97/200\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 98/200\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.6931 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 99/200\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.6932 - accuracy: 0.4900 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 100/200\n",
            "500/500 [==============================] - 0s 397us/sample - loss: 0.6933 - accuracy: 0.4640 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 101/200\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 102/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6932 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 103/200\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 104/200\n",
            "500/500 [==============================] - 0s 463us/sample - loss: 0.6933 - accuracy: 0.4760 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 105/200\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.6931 - accuracy: 0.5220 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 106/200\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 107/200\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6934 - accuracy: 0.4380 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 108/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6933 - accuracy: 0.4660 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 109/200\n",
            "500/500 [==============================] - 0s 466us/sample - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 110/200\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 111/200\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 112/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 113/200\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.6932 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 114/200\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 115/200\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 116/200\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.6931 - accuracy: 0.5340 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 117/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6933 - accuracy: 0.4840 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 118/200\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6933 - accuracy: 0.4660 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 119/200\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 120/200\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6933 - accuracy: 0.4880 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 121/200\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 122/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6933 - accuracy: 0.4420 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 123/200\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6932 - accuracy: 0.4840 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 124/200\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 125/200\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6932 - accuracy: 0.4880 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 126/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 127/200\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 128/200\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 129/200\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.6932 - accuracy: 0.5300 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 130/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 131/200\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6932 - accuracy: 0.4840 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 132/200\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 133/200\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.6932 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 134/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6932 - accuracy: 0.4420 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 135/200\n",
            "500/500 [==============================] - 0s 397us/sample - loss: 0.6932 - accuracy: 0.5080 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 136/200\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.6932 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 137/200\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6932 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 138/200\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6932 - accuracy: 0.4820 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 139/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 140/200\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.6933 - accuracy: 0.4600 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 141/200\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.6932 - accuracy: 0.4720 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 142/200\n",
            "500/500 [==============================] - 0s 393us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 143/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 144/200\n",
            "500/500 [==============================] - 0s 397us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 145/200\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 146/200\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 147/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 148/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 149/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 150/200\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 151/200\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 152/200\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.6932 - accuracy: 0.4440 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 153/200\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 154/200\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 155/200\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 156/200\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 157/200\n",
            "500/500 [==============================] - 0s 488us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 158/200\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.6932 - accuracy: 0.4480 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 159/200\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 160/200\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.6932 - accuracy: 0.4760 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 161/200\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 162/200\n",
            "500/500 [==============================] - 0s 437us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.6933 - accuracy: 0.4760 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 164/200\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 165/200\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.6932 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 166/200\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.6932 - accuracy: 0.4600 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 167/200\n",
            "500/500 [==============================] - 0s 455us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 168/200\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.6932 - accuracy: 0.4520 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 169/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 171/200\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 172/200\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 173/200\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 174/200\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 175/200\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.6932 - accuracy: 0.4600 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 176/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6932 - accuracy: 0.4720 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 177/200\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6932 - accuracy: 0.4840 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 178/200\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 179/200\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 180/200\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.6932 - accuracy: 0.4900 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 181/200\n",
            "500/500 [==============================] - 0s 455us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 182/200\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 183/200\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.6932 - accuracy: 0.4760 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 184/200\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 185/200\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 186/200\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 187/200\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 188/200\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.6932 - accuracy: 0.4760 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 189/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 190/200\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.6932 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 191/200\n",
            "500/500 [==============================] - 0s 470us/sample - loss: 0.6932 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 192/200\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6932 - accuracy: 0.4540 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 193/200\n",
            "500/500 [==============================] - 0s 385us/sample - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 194/200\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 195/200\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.6932 - accuracy: 0.4780 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 196/200\n",
            "500/500 [==============================] - 0s 453us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 197/200\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6932 - accuracy: 0.4780 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 198/200\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 199/200\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 200/200\n",
            "500/500 [==============================] - 0s 396us/sample - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JtpI2okoLV8",
        "colab_type": "code",
        "outputId": "1eca2da6-bab6-4526-ff74-fd2a54f83bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print('Testing Accuracy: {}'.format(model.evaluate(Xtest, Ytest)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164/164 [==============================] - 0s 151us/sample - loss: 0.6935 - accuracy: 0.5000\n",
            "Testing Accuracy: [0.6935200037025824, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjAa9I1XvIVU",
        "colab_type": "code",
        "outputId": "2131c8b5-c741-4f8f-acbc-e229269a10cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "Yp = model.predict(Xtest)\n",
        "print('Training Accuracy: {}'.format(model.evaluate(Xtrain, Ytrain)))\n",
        "print('Testing Accuracy: {}'.format(model.evaluate(Xtest, Ytest)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 0s 140us/sample - loss: 0.6931 - accuracy: 0.5000\n",
            "Training Accuracy: [0.6931473612785339, 0.5]\n",
            "164/164 [==============================] - 0s 176us/sample - loss: 0.6931 - accuracy: 0.5000\n",
            "Testing Accuracy: [0.6931473656398494, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqi_4X0Lv7t2",
        "colab_type": "code",
        "outputId": "b9a6c5cf-5f99-463e-f246-7b6f5f24b26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result.history['loss'], label='loss')\n",
        "plt.plot(result.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydeXxcZb3/39/ZM9mbpVu6r5QWWigg\nQpFFoAqKGyCKAgrcyxVcUK54VeSi/BS46hUvV0TZZStwUZClRSiU0oJNS/c13dM1SbMvsz6/P86Z\nyWQySZO2SU7b7/v1mnZycuacZ85kns/5ro8YY1AURVGUnuIa6AEoiqIoRxcqHIqiKEqvUOFQFEVR\neoUKh6IoitIrVDgURVGUXuEZ6AH0B8XFxWb06NEDPQxFUZSjiqVLl1YbY0rStx8XwjF69GjKy8sH\nehiKoihHFSKyPdN2dVUpiqIovUKFQ1EURekVKhyKoihKrzguYhyKohx/RCIRKisraWtrG+ihOJ5A\nIEBZWRler7dH+6twKIpyTFJZWUlubi6jR49GRAZ6OI7FGENNTQ2VlZWMGTOmR69RV5WiKMckbW1t\nFBUVqWgcBBGhqKioV5aZCoeiKMcsKho9o7fXSYVDOWpZu7uBpdtrB3oYinLcocKhHLX8et4G7npl\nzUAPQ1G6JCcnZ6CH0CeocChHLW3RGG2R+EAPQ1GOO1Q4lKOWSNQQiatwKM7HGMNtt93G1KlTmTZt\nGs899xwAe/bs4ZxzzmH69OlMnTqV9957j1gsxrXXXpvc97e//e0Aj74zmo6rHLWEYnEiMRUO5eD8\n5ytrWLu74Ygec8qwPH72mRN7tO///d//sXz5clasWEF1dTWnnXYa55xzDk8//TQXX3wxP/7xj4nF\nYrS0tLB8+XJ27drF6tWrAairqzui4z4SqMWhHLVEonGiMTPQw1CUg7Jw4UKuuuoq3G43gwcP5hOf\n+ARLlizhtNNO49FHH+XOO+9k1apV5ObmMnbsWLZs2cItt9zCG2+8QV5e3kAPvxNqcShHLRG1OJQe\n0lPLoL8555xzWLBgAa+++irXXnstt956K1//+tdZsWIFc+fO5cEHH2TOnDk88sgjAz3UDqjFoRy1\nWMKhFofifGbNmsVzzz1HLBajqqqKBQsWcPrpp7N9+3YGDx7MDTfcwPXXX8+yZcuorq4mHo/zxS9+\nkV/84hcsW7ZsoIffCbU4lKOWSMyoxaEcFXz+859n8eLFnHzyyYgI9957L0OGDOHxxx/nvvvuw+v1\nkpOTwxNPPMGuXbu47rrriNuJH7/85S8HePSdEWOO/Tu2mTNnGl3I6djjtLv/QX1LhI13f2qgh6I4\nkHXr1nHCCScM9DCOGjJdLxFZaoyZmb6vuqqUo5ZILE44Fud4uPlRFCehwqEctUSilikfi6twKEp/\nosKhHLWE7fiGBsgVpX9R4VCOSowxScHQ6nFF6V9UOJSjklQrI+GyUhSlf1DhUI5KUtNwoxrjUJR+\nRYVDOSpJFY6wWhyK0q/0qXCIyGwR2SAiFSJyexf7XCEia0VkjYg8nbL9HhFZbT+uzPC6+0WkqS/H\nrziXsFocyjFId+t3bNu2jalTp/bjaLqmz4RDRNzAA8CngCnAVSIyJW2fCcCPgLOMMScC37W3XwKc\nAkwHzgB+ICJ5Ka+bCRT21dgV59MhxtGD6vEH393Mhr2NfTkkRTlu6MuWI6cDFcaYLQAi8ixwGbA2\nZZ8bgAeMMbUAxpj99vYpwAJjTBSIishKYDYwxxak+4CvAJ/vw/ErDiY1IH4w4QhH4/zq9fXUt0b4\n4ezJfT00xYm8fjvsXXVkjzlkGnzqV93ucvvttzNixAi+9a1vAXDnnXfi8XiYP38+tbW1RCIRfvGL\nX3DZZZf16tRtbW3cdNNNlJeX4/F4+M1vfsN5553HmjVruO666wiHw8TjcV588UWGDRvGFVdcQWVl\nJbFYjJ/+9KdceWUnJ06v6EtX1XBgZ8rPlfa2VCYCE0XkfRH5QERm29tXALNFJCgixcB5wAj7dzcD\nLxtj9nR3chG5UUTKRaS8qqrqsN+M4ixSxeJgdRxNoSgAzfb/itJfXHnllcyZMyf585w5c7jmmmt4\n6aWXWLZsGfPnz+f73/9+r7sfPPDAA4gIq1at4plnnuGaa66hra2NBx98kO985zssX76c8vJyysrK\neOONNxg2bBgrVqxg9erVzJ49++AnOAgD3eTQA0wAzgXKgAUiMs0YM09ETgMWAVXAYiAmIsOAy+39\nu8UY8xDwEFi9qvpk9MqAEUqxOKIHsTga2yIANLWpcBy3HMQy6CtmzJjB/v372b17N1VVVRQWFjJk\nyBC+973vsWDBAlwuF7t27WLfvn0MGTKkx8dduHAht9xyCwCTJ09m1KhRbNy4kTPPPJO7776byspK\nvvCFLzBhwgSmTZvG97//fX74wx9y6aWXMmvWrMN+X31pceyi3UoASxh2pe1TiWU9RIwxW4GNWEKC\nMeZuY8x0Y8yFgNi/mwGMBypEZBsQFJGKPnwPikPpkFV1UOGwBKNBhUMZAC6//HJeeOEFnnvuOa68\n8kqeeuopqqqqWLp0KcuXL2fw4MG0tbUdkXN95Stf4eWXXyYrK4tPf/rTvP3220ycOJFly5Yxbdo0\nfvKTn3DXXXcd9nn6UjiWABNEZIyI+IAvAy+n7fNXbOvBdklNBLaIiFtEiuztJwEnAfOMMa8aY4YY\nY0YbY0YDLcaY8X34HghH4/xpwZbkXaviDFLdUwdbBTDhqmoK6Weo9D9XXnklzz77LC+88AKXX345\n9fX1lJaW4vV6mT9/Ptu3b+/1MWfNmsVTTz0FwMaNG9mxYweTJk1iy5YtjB07lm9/+9tcdtllrFy5\nkt27dxMMBrn66qu57bbbjsj6Hn3mqjLGREXkZmAu4AYeMcasEZG7gHJjzMv27y4SkbVADLjNGFMj\nIgHgPREBaACutgPl/c5LH1Vy92vrGF6YxaenDR2IIRyTxOOGmDF43Yd279IxxpHZ4nhi8TYunDI4\n6aJq0hiHMgCceOKJNDY2Mnz4cIYOHcpXv/pVPvOZzzBt2jRmzpzJ5Mm9T9j4t3/7N2666SamTZuG\nx+Phsccew+/3M2fOHJ588km8Xi9DhgzhP/7jP1iyZAm33XYbLpcLr9fLH/7wh8N+T30a4zDGvAa8\nlrbtjpTnBrjVfqTu04aVWXWw43ed9HwEMMbwyMJtALSGY315quOOxxdv4+GFW1n4w/MP6fXhgwTH\n61rC3PG3NTSFogzNDwAa41AGjlWr2jO6iouLWbx4ccb9mpq6Lk0bPXo0q1evBiAQCPDoo4922uf2\n22/n9ts7lsxdfPHFXHzxxYcy7C7RyvFuWLS5hg37rNz/kFYnH1E27G2ksraVtkjXgmyMoaELF+HB\n0nFbbKGvb4moxaEoRxgVjm549P2t5Pgto6y7CU7pPdVNIQAaWruOO7xfUcPMn/+DqsZQp991iHFk\n6I6bEI66lgiNtmA0qsWhHAWsWrWK6dOnd3icccYZAz2sDgx0Oq6jufbjY7hoyhD+/cWVtEVVOA5G\nVWOIf3mynAe+egpD87O63be6KQxAfWuE0jzLlfT4om3MGFnASWUFAOw40EI4FqeqMURJrr/D6zvE\nOKKdXVUJoa9tCVPU5gMsqzEcjePz6P3S8YIxBjtWetQwbdo0li9f3q/n7G0diX6DuuHsCcV86dQy\nAEKRo99VFY+bI9YQMFMx3aZ9jSzbUcfKyvqDvr6m2bIi6lMsjrtfW8eLSyuTP7eErXNkEu3U95Fp\nPY6kxdEa6WBpqLvq+CEQCFBTU6NLCx8EYww1NTUEAoEev0YtjoPgcgk+j6tXFkf6nbMT2N/QxrWP\nLsHjFv7vpo/jOcRsJoAFG6u4/vFy3r/9/A6WQMi2Auq7cT8lqG5stzgAQtEY4Wi8QyypOWRd87YM\niQkdguMZxLA1khLjSBGLprYog7J9Bx2fcvRTVlZGZWUl2jni4AQCAcrKynq8vwpHDwh4XL2yOO5+\nbR2fmz6Me7/kDOFoaIvwxQcXsbe+jUjM8JcPtnPtWWMO+Xird9cTjsXZ39jWUTjsa9Rd3AIsSyI5\nsbd2rOpOtSQSFkdrhvjSwdbjaLVfW9ca7mBxNGotx3GD1+tlzJhD/ztXukZdVT3A73UT6qHFkbhz\nrqxt7eNR9ZxVlfXsPNDK/V+ewawJxfz6zY3UNHUOOAN8sKWGn/99bcbfJdhdZ7239ISBcIrFEYrG\nuPPlNRnPU2PHNxL7QnvguoPFkXBVZRDtg1WOJ8SmriVCUyhCws2tKbmKcviocPSAgNeVcfLKRMK9\nkkk45izZyVvr9h3RsfWExKQ8YlCQH1w0ica2KAsrqjPu+8bqvTy8cGu3/Z9211ntEVrDHfcJpVgR\na3Y38NiibRnPU50iJt0JR4t9LTNbHN1XjidiHKGoFVwvzvF3OI+iKIeOCkcPCHjcPU7HTdzR7q5r\nJZbmQnngnQqeWNz79gKHSyKQneP3MLooG2jPakonUTdR1427aZctiglXUoJUi6O2OWzv0/m6VWe0\nONpjHclxd+OqSri0RDLXcaQWbFbWtrYXAWpwXFEOGxWOHuD3unpcAJjwoUfjhn0NHRuX1TaHqWvJ\nPGH3JYkJOCfgIS/Lg8clXbqqEvGJ7saZcFWlT+iJGEd9a4QaWzgyZV8lzu11S1I4GjJZHAmroYsY\nh9cteN2uzK6qFOEIReMMsVN+G1U4FOWwUeHoAakWxysrdne7rkOqDz3VXRWNxWloi3LAnpD31Ld2\nEpa+IuGeyfF7EBGKcnwd3EWpNLRa+9a2ZLY4GtraC+rS27CkWhwHurE4EqIyclAwKVQJSyCcQTgy\ntXuxhMOFz+3K6KpKF7VhBVZdicY4FOXwUeHoAQGvJRy76lq55ZmPeGXF7i73TXWFVNa2JJ8n7qxr\nm63/b31uBf/+wso+GnFHmkNR3C7Bbxe+Fef4OwSoU0m4qhKupnR2pYhh+uScmPRThaM53HmirmoM\nkev3UJLrz+CqSk3H7S6rymqQ6HFLty1HEhTn+PC4pMcdct9ev493Nuw/+I6Kchyi6bg9wO9xUdsS\nT94dd3W3DunC0T7J1qXcWYejcXbWtuB29U9Fa3MomrQ2AIpy/FR3IQz1SVdV5gk24aaCzpNzIj7R\n0BpJClMiwJ1KTXOYohwf+VletlVb4toeHG/fP3H8TIkJoahlcVgxjsyV4y6BRJgpN+AlJ+DpscXx\nu39sAuDcSaXsPNBCKBpnfGmf9tRUlKMGtTh6QMLiSASDa7qYdKF9AvS6pYPFkRozqG0JU9UYYk99\nW79UtTbawpGgONtHdYb+T9Ae46jtIsaRKhyd0nE7WBzW8buKcRTn+MnP8nayOHpTx+H3uPC6urY4\nUmtMcvwecvye5OezYGMVZ9/zNvsbM7sL61oj7LLf60//tpqbnz78NQwU5VhBhaMH+O103ESqbVdu\nHGi3OMaV5HSwOBIuKrB6MCX6Jh3o5lhHiuZQlGy/O/lzUY6PmuZQJ9GKxuI0p7TqyMSuujZ8bhdB\nnzuDxWFN4JGYSU66mVxV1U2hpMXRbR1HonK8u+C4x9UhdbimKUQ8bmiNxCgM+pJ9qXICtnDYn8+K\nnXVU1rbyx3e3ZHyfdS0RqpvCtEVibKlqpmJ/U5frfijK8YYKRw/we9yEovHkHfCBLtw4YAVfXQIT\nBud2FI6UO/iNdqt2gD311h1vJBbnj+9u7pOVBptDsY4WR46ftki808SfurRqV1lVu+paGVoQIOjz\ndBnjANhWY1lbGYPjTWGKcvzkBby0RqyCyca0yvGYPflDd8LhwuOSpKtqd10rZ/7qbeat3UdrOEbQ\n56Yw6AUg1+8hL+BNuqqqbHfjXz7Y3snqiMfb27nvONDCrrpWonHD9prmjNdEUY43VDh6QMDrIhSJ\n0WTfASfcMJlost1CIwqzOtRypMYMNu1rX6wlIRzz1+/nl6+v5+31Rz4g2xiKkp0iHEV2MVx6rCa1\nVUiqhZTK7rpWhuVnkeVzdcp2SrUWEgKQ7qqKxQ0HWsIUZ/vItyf1+tb2TK1EjCNVlDLXcVjBca/b\nlbQEFm+uIRyNs72mmdZIjKDPQ0GW1ZcqGeOwz1PdFKIw6CUaNzz6/rYOx25si5IwxpZsO5D8DDfu\n63qRnb6kORRlT71zOhEoigpHD/B73LRF22McXU2qYE06uQEvo4qCHe5S61rb7+A37E21OKwJISEY\nqQITicU7FdkdCs2hKLmBVOGwJtP0IsDURZO6i3EMK8gi6PV0TsfNUOuSsDgq9jdaKcmtEYyBwmzL\nVQW2cKRkVRljaEkRnC7TcT0dheOfWw8AlputJRwj4HUnxSnhqkoIR1VjiElDcpk2PJ8VO+s6HDv1\ns1q8uSb5fNMACcdv39zIZ//nfeIZenIpykCgwtEDAl4XkZhJulNqurU4IuT4PcwYWQjA0u21gFUX\nkZi8N+3v6KqKx01SOFJjHve+sZ7P/H5hp3M0tkWSx+0JzaEo2b524SjpwuJIxBsGZfsyZlXF4ob9\njSGG5gcI+Ny0pBcARmOkL33QHI6yv7GNi//7PV5fvTc5cecGvOR1EA5ruzFW8WRzili0ZRCkSCyO\nzy143ZJscrhkmy0cLVZsItVVleP3kBNoD44n2pCMK8mhYn9HQUjt7vvBFks4gj53h8+tP9lc1URV\nY4gt1QMjXIqSjgpHDwh4rcByYlJvi8S7XIO8KRQlJ+BhfEkO+VleyrdZE3xdS5jSXD85fg/VTWE8\nLmF4QRZ76lpZs7uB/XaWU2psYe6afWyuau50rscXbePyBxf1OLDe1JbuqrIsjvRajkTx38hBwYwW\nR3VTiFjcMDg/QNDr7tTuPBSNU5TSsrw0109zKMa+eut1+xraUooR3UmLoyHF4kgcJ+HicrskY1v1\nZIzD7SIctTr1bqm2rLva5ggt4ShZXneKq8pDbsCTPE91U5iSXD/jS3PY3xjqYG2limZ1Uxif28Vp\nowd1Epj+ItEbbNn2uoPsqSj9gwpHD0gUzqVO1Ae6cOU0tVkxDpdLOHVUIeXbrbvg2uYIhUEfhdnW\nZFmc42dYQYA99W28tX4fIlAY9CYrtnfUtLDjgBVgTk3rBdhc1UzcwIrKg08kxhiawx1dVYn1KNLb\njiQmz1FFQepaIp2yrvba8ZgheQGyfG5aIh3daKFoPNlMEKCsMIvmUDQpQo1t0aTFkeP3prmqonjd\nlrkSjrYH7guDvi57VSUrx+OGJVtr7eN6qGsN0xqOkeVzMzg/QI7fg9/jItfvIRSNU9cSpikUpSTX\nz7gSq3fXlqr2wHcioyxhrZQVZjF5SC5bqpq7bf7YVyRSoD/a2XMrU1H6EhWOHpCwOFLrNw40hVmy\n7UCn4G+jbXEAzBxdyOaqZmqaQtS2hCkI+hgUtCbtklw/Q/Oz2FPfxt9X7uHUkYWMLMpOTrLvVbQv\nPrMzTTgScZN033wmWiMx4oYOFoff4yY34OnSVTVqUJBwrHPW1V67RcrQfEs4MsU4inP8SXfVyEFB\nQtF40rVnCYd1jpyAJykcB5rDtIRjFGVbohOKxpJpvMU5voxZVeGYwedx4bUrx5dsO0CW183pYwZR\n1xKhNWIJx/WzxvDCTWciIowYFATa3YfFOf5kUV+qNZG4DicOywesrsITBucSjsXZbov5XtvF2Nek\ntng5UhZHNBZPxqMWbKzi6Q93HJHj9oS+yBpU+h8Vjh4Q8FqXKbV+Y/3eBq7442Ke+rBjt9umtii5\n9iR92uhBgDVR1bdGKAx6KbCFozjHx9D8ADsOtFCxv4krZo6gMOhNukkWbqpOHie9RXvCEulOOMLR\nOM+X70y6hlKFA6w4R3r1eENrBLdLGF5o9XVKd1clLI7BeQGyvO5OFd2haJyA150cd2KiTrhamkKR\nZGZajr9dOBLCWJxrrw0eiScrzgdlp1kc834Kb95hxzgsV1WibmRUUZBiuw9XJGYIet3kBbxMHpIH\nWLU10B7wLsn1M3JQEK9b2FyVIhz2+54yzHrdyEFBJg62XrtuTwM7alqYde/b3PLMR31ugeyxr934\n0hw27m/s4FI7VL719DK++IdFtEVi/PDFlfzs5dU9WrXxcNnX0Mapv/gHf3hnc5+fKxPxuOHfX1jB\n+3ar/+01zdot+RBR4egBfk97jCPbZz1/b1M1xnTOtGlKqdKeNjwfn9tF+fZaalvCFGb7km4iy+Kw\nOrbm+j1cevJQCoM+alvCxOKGRZtrmD11CH6Pi50HWjocv7opjAisqKzvsvJ8YUUVt72wknc3VCXP\nkUpRji+jqyo/q13c0gPkexva8LqFomwfWV5357bq0Rh+j4v8oBe3Sxhiv7+E8DW2RZN1FLkBD163\nizHF2XywxXLnJSyOcCyetDiKcvwdLY4Nr8PSx4lGLdeWz86qamiNkJflpTDoS2aLZfnaix4Bxtpu\nqcV2wLskx4/H7WJ0UTabUyyOupYIQZ872YJ+5KAgJwzNI8fvYdHmGt7duJ9IzPDqqj38x0urOl3D\n9Hb6mVi6/QD3v7UpKcYJjDG8vX5f0iLYbWfdXXrSUIyBlTut9dzrW3t2ngPNYb719DJO+fmbfPOx\nJWyuamLumn2s2lXPjU8uZY+9KuS8NXsPeqzD5b1N1YSjcX49bwMre+BmTef9imo+2mFZi5FYvNcF\nmQs2VTGnvJIH391McyjKpfcv5OevdL9omZIZFY4ekLA4qptCybvoRZutu5at1e2+8Vjc0BKOJV1V\nAa+bU0cVMnfNXtoicfLtiQ0s4RiSb93Zf27GcII+D4VBK5tpa3Uz9a0RzhhbRFlhFjsPtFscO+zC\nuo+NKeJAc7jLlQYTlsbaPQ1AZ4ujKNvfOR23NUpewJMcY7rFsa++jdLcAC6XEPS5OxcA2m1AEu8z\nIaCJKvKmUIqryv7dKSMLWWePMRG0tywOWziyfURixrqzNwYadkNbHaMjW5JNDhOdh/MC7aIHnYUj\n6PMwvCAreU1K7ZYk40pyqKjq6KrKz/JSZlteI4uCeN0uzhxXxIKNVby3qZrhBVncdO445pRXsqii\nmtW76vnawx9y8n/O45GFWwFLHL7x2BI+cd/8DkkPP35pFV/8w2J+8+ZGZv9uQYdmiu9srOIbj5Xz\n/NKdQHt849PThuLzuPjLB9up2N/Ex3/5Fpfc/x7LD+KunLtmL6+u3MP40hzeWr+ff31yKV63MHlI\nLgs2VjFpcC7DC7J4ddWebo9zqLSGY/zhnc0caA7zfkU1hUEvJbl+vvvs8l6lmrdFYvzrk0v5yp8+\n5M21+5j93ws49edvcufLa7o9TuqN1ZP2WjiLN9cwp3wnjaEor63e0+PVPbti4aZqfvH3tdz1ylrW\n7204rGMdLahw9ICAbXGEonGG5AdwuyQ56W5LqSZuD/y2T9KfnDKY7fZkXxj0McgOjpfk+JkxsoCT\nRxRw3Vmj7d97aQpFk2I0qijIiEFBdta2ULG/kTnlO9lxwPrdZ6cPA7oOkLeEY+TSwvo91h1qassR\ngNI8f6e27vXJu3ZrjJksjsF51mQbsF1VqX7+YLiWK/f+F79s+im/lP9hWIN1N77LdkU12BaHiJXe\nCnDqqMLk6xNpwuFYLJmOm8jSaovGoa0eItb7Pzm2KqWOw9gWhyc5doAsb8f3DJbLxxhrAaiE9Teu\nNJsdNS3JO9g6WzjOHFfEHZdO4dxJJQCcM6GYytpW3tlQxawJxXznggkML8jiJ39bzVf//CHr9zYy\nKOjjvYpqQtEYX3/4n3y0o5btNS38bbnVUbm+JcKzS3by+RnDefXbZzMkL8B3nl2eTLx47p+WYLy9\nzhKTPXVtuF3CuJIcvvvJCbyxZi9X/ekDfB4XtS1hvvSHRbz0UWWn95lg6fZaBmX7eOaGj3HyiAI2\n7W/iMycN4+7PT8XtEr51/nguPWkoCzdVd9ktIBSNJW9YuiIcjbNhb2MnC/i+uRu45431/Pc/NvJ+\nRTVnjS/m15efzNaaZv7fa+uS+72+ak/yBiMTb6/fT2PI6spwwxPl7G8MMWtCCY8t2sYD8ys67b+j\npoWvPfwh5//6XXYeaGHngRbe3rCfcyeVEI0b7n1jAz63i8a2KAs2VjN3zd4OVtCizdVc8Ot3WL2r\nvssxtUVi3PhEOVc//CFPfrCdv3y4nS/87yL+sbZnq3zua2jr8QJxTkO743bHxnkQj+DPOjO5Kcdv\nTU4J4ahuCifvUNtrFNov64UnDE6u4V0Y9BK3v1jFuX4G5wX427fOSu5bYE9kiT/WssIsygqzWLa9\nlnve2MCba/dx9cdGkk0rnyqt5WduYdWuei6dUswDb2/g0uqHGbVnLpz4Oc6qWMWqwHvs2DOEXd5B\nnPyGFy69B0ZZ72VwXoDGtigt4ShBu8ajs6sqjDGG7z23nAtOGMze+jYmD80F2if+tqhVoU2kjXui\n93Bi3VbaiqZwQv0SPG/P5wHv6fyq7jognyY70Jvja+/UmyociYysUMQqfHS7JFnA1xqOkdOyK7nv\nKfHVzHe7MMYQiVnp0ZbF0S4cQV9m4Xh3YxWDgj48buu+aeLgXKJxw/KddZw2ehD1LREKgl68bhff\nOHsMxCLQWM15w2K4iRGOwdkTigl43dz+qcnc8sxHDMkL8Py/nsn/vlPBa6v2smJnPc3hGL+5cjr3\nv7WJ55fu5JqPj+btDfuIxQ1fP3MUJw7L5/6rZvCp373Hr+dt4HsXTuQf6/bh97hYWFFNazjG7rpW\nhuRZNys3zhrL3NV7WVFZzyPXzmTm6EH8yxNL+d5zK/jn1gOcOCyfZ5fs4NKThvGvnxgHwLIdtZwy\nsgC3S/jVF6Zx89PLuPETY5k8JI+lP/kkBUEfY4uz+eOCLXzz8XKu+fhoBGtSi8UNM0YW8p+vrGHd\nngYe+tpMRhcHeWXFHkrz/MwaX8KQ/AD3v7WJv3y4nbqWCD+cPZmbzh1HPG54d1MVjy7aSq7fw1Mf\n7iAWN5w9vpiPjy/mhlljeWjBFs4YU4TbJfzbU8sYW5LN3751FrkBb6fP7aWPdlGa6+eRa0/jt29u\n5PsXTWLKsDw8z37Ewwu38vUzRzM4L4AxhueXVvKzv63B7RJcAl96cBEuEVwi/L/PT+PyBxezq66V\nm84dxzP/3MEvXl3L9poW/FbC4R8AACAASURBVB4Xf7j6FE4ZWcj3nlvOvoYQP3h+BS/ffDY+j4vf\nvLmRpz/cQVskxg8umsiGfY3MW7uP2y6exPWzxlDXEuGGJ8q54clyfvzpE/jm2WOIxAwLK6oYVZRN\nYdDHbc+vYEt1M+FonF11rQwvyOKhr5/KiEFB/B5X0i0O1o3o0x9uZ2t1C1OHW65Sj8tFUY6Pk8ry\nCfo87Kprxe9xUZTtS36nwKpTemvdPuZv2M9/Xzmjk/V9uKhwdIUx8MH/wpb5jJh+CzPFmuCmRGrx\n+BrZIW2MK81h8/4m9q3N4q+baxhWkMV02czwZg9s3Q6hRkaGGrmxcBN76ls5af07tMS9zHZ5mbpz\nA9TGwO2BnCEQaeHsTR/xTXcc78YJfMrTQOmuOOdEdtMc3kp4g59LXTFGLnuGRYG3yX+smTd8I8j5\nCPhwJ99KjHvUWfDhHxnsCvDH6CWMl93kSTPe1np46nK49DcQLGJaqJYvuJYQfWUeBLMgp5RpDWEK\ni0oprIkz3VWBf+teKlsN3lUbWL27lNIGP6eWDYWdccY0buFq92J4658QroHdHzGdDbw49m6++LWb\nIdTEnnm/5ZPlv+Niylnmm0B2U4zi1REudhXCe2tgzDlMiMU4z7+BcCTMxFCEU2Qr1A+jJewj6HMn\nrYa2SMxyUwGUTuGUfWt53xUj7nbRFokRDTUxKbqesuZ8ZojVEr203gMbVkC0DfKGgbj4uHs7W1zr\nmOFrhvnl4PEzW/zcFNhI60vPwqgsrq+pJivgh5eegOb9sOMDCDdRBmwIuGgwQQpe88J7Q7i0cBQT\nJhVROmoyg7Zs55t1i5kSOUDtq3/n6244u7aSvNJanlrZxPYljaxfWc+snEZOdm2DXVuZaAy3T2vi\n9SVvce+2DzjJNHHdx0bx+PtbWfdBjLz9uzk3S6AyH4/LzeMXRNlZFWVaTiXs2cDjs1p4JLuR+cvm\nU1newmh3mA3veIgM+zhtriyyqtdw4YSRsPsjTgDeuiIIVfNgVxMF3iB4g0z1BXn4vChP/fMjHnv2\nww5fg7lAoc/DpYU+/vxMBXFDMiHgJZdQnJvF7voQV44rIRI3/H3u6+xaV8Dm/U00tkU4Py/AD2dP\n5vtzloPAubmFsKuaH0yNUV+xn4fnvEiWx8WnCr3sPdDGz/+4haJsK8GhsS1Krt/NpEIhu2Ird431\nMXXfHh4+ZySYCtjazE9HbsO9ej2P/Wk5QwflseVAiG01rXxzSJBrzz+J+piP+15dyeg8F5/52CCG\n7ZrLTSPqeKa+iatHDiK7up55a/ZwzYgcWkNhHnpiNdkew5R4jJ+cOoSXPyrn+SdXMqnUz4FFa7i9\n2BCINbH8NT+NZhD3Th/HFeNLoLqVwQhzPpfHf83dzQuvzeWdd31EjaG+JYzLJdbNZVuUC8cNQoCx\nU3N4ZfkWfvD79QB4XS5GDMqivjVCUyhKPA6ReJxsn5sVSzq64/weN0U5vqQr0+9x4/MIg7L9+D3C\nxv1NYKA0L8DO/aOYWFZy5OZGQPqjrfdAM3PmTFNeXt77F0ba4NVbYflTR2QccZcPMVHEZA7qxdwB\n3LHuVwWMGWGp/3ROP/8LrHvrcULubE487Xz+MH8DzcPO4kf/diM07uP+BTv4zXvtvvPyb0+h+PnP\nQ+3WDseLenPwuFwQOjTfrBEXEiyCovHcvmUaRbO+yW0XTwasFNfrf/ssX3Iv4CzXGpolSHZOPnmt\nlYyLb+3ymHHxcOfkvzNvcws/vuQEbnnmI9783jlMqHwRXvkOXHgXvHkH/zfxXlbnzmL+okU86r2X\n0a6euQi6osEECRaUUFXfTNAD+dlZECiAEadB6RQQYdnqtbTUV3P2hBJo2AO126yH7UKL+fKpD8Uo\noBmXHPvfreMR4/Yjsa67RziOby2BkomH9FIRWWqMmZm+XS2O7vAG4LIH2Dfpam59cgECXHbyMPbU\nt1C+7QD3felkfvjCCnweF+FkgM3wk0tOYPywEvDngi+X6sYW/rp0G9d+9iI88TDUVEDecMgqhHjE\nmoDcXja0FPC137/KYKnl5LICfvmFaWza38INz65mXIGHy2YM47a3m7l48mhO/9gM/nvTTLZVt/Dn\nGTP5zZvzmUGBNYTcwdTGqzu8lWDxCPjXhbB/HZgYlQca+cZzm/nWZZdw2YwR0FbPF3/xJF+YVsBX\nTx3Kqh1V/OzNPVSTz4iiHOpr9lEs9fzLOWM5c1wJi7c38r1/NPHEdy9j4pB8orE4z/74dW5NMbWz\n/W62maH8V/RK/sve9vGyIlqzY7z0lRGwdzW4fby8pprnl+7m5589gfl/e5TrPHOJhRoJ+n1Ji6M1\nYXGIi/iMa9gw9898puIORpZcynd8rxHBw+IZ93LC6BF857nlANz5mRMZM6IMPD7rGgMNMS9ff2Il\nM048gZ995QKIhSHSyp66Zs7+n5VcP3ksj72/jWtPH82PPn1Cpz+JU07L8HdiDDRXQ6QFyRvBJ+56\nk5ZQiBtnFvLD2ZMA+PsHa3j6nY/Ijjfy7fMnMK2sAMRlBVsQEMGSGUFE+N38zXy0o45YHC6ZWsqX\nTx0K8Sh4g+D2WUJvWwy0HrCO5csh5s3mG498wKQiF5MLhbmrdnH/VdPxezzWucQN+WUQyIdIqyV4\n4RaItAC9FDpj/2Pi7Q/Ses5Ie1Fn1BiCXk+H30Xj1nIF+Vm+jK8DCLuzqIllM3TocAg3QX0lhJvB\nE7C+RyKWOzEesf43BlxuOybWYu3nCVjfZ0/A2t5am7zuIODyWK9J/V/c4HKzdGcjr67Zz3UXzGDE\n8DLE44OWA9C4F0KN1iPSfQyoUy+eXnMYr88bepjn7owKx8EQwVV2Ku/HrTTAswZPpjkvzOYDeyid\ncR6b/2FlPRWk1GD8ePw5MDg3eYjiYrh+zEn2TwEYfkrKCXxQPB6AQlcrNeRTY/KZOrgMhp5EaUGE\nXa4DfHHmBM46YyRmwdvJmoLiHD9LttUmW4SndrdNLc5ziR0olhzr7hnIK42w0YTZ12i3UXHnsDQ6\nhvNLJ8H48Zw41lC37F12VDfz7xfN4JevrWd1XSs3TfwYjC2iNbaPvZTTGrEmm8R644n1L4Bk7AQg\nL+ChoS3K3oY2hhdkQcFI6wHMHhNnxjmWpbXevGWNJxwh2xdM+mbbInFo2AU5g4n4cvla+Ef8I/uX\nnLLvJf4Rn87Po1fz43GfJmtyCe/aAfvYuHOg1P4chkyzxgFQ5mPMuOH2BJEF3iyGBgdx/uR9vFBe\nSSgaT/bR6hEikGO5AlzASSPyeb+ihqkTx0NOKQCXfrKUSdNm8s6GKk44azS4O+elpE4NXxl8Nguf\nWsqSbbVcNGYqTBzVo6G4gWmn+fmf+RUEdrsYP2Qi/qmzev5e+gif/UjHA+T34LXtU18pDBp75AbW\nA04dBqeekbYxOMh6HKf0qXCIyGzgd1h/z382xvwqwz5XAHdi3b+sMMZ8xd5+D3CJvdvPjTHP2dsf\nBmZifc82AtcaY/q0iVAiHResu+jrzprIjbPGIiKMKc5h54FWvnvBBO6bu4HmlHTc3lKYkko6vMBK\n+83P8vL6d85hlJ0S+vp3ZiXrP0py/RxoDieLxFLX00it+s5OWTY2Qa7fQ9DnZm+9JTqJosJhBdax\nXS7h2xdM4H/fqeCTJwymfFstjy3alqzNSFTTJ84TsosB/R2Eo936GDEoyJrdDeypa2NSiqiCJTYj\nBgXZ39BG3J4+Q6EwQZ87ee2TFkfeMCIxQzX5/PW0pwiH2vh/71Ylr5Xf404uMpXly/w5/DUlISGV\nz00fzpt2RkxqkL23nDpqEIs313D6mI4Ty4TBuUxIe+9dUZLr56nrP8brq/dw0ZQhvTr/N84eQyRm\nBV8/Pe3I320qSp8Jh4i4gQeAC4FKYImIvGyMWZuyzwTgR8BZxphaESm1t18CnAJMB/zAOyLyujGm\nAfie/T8i8hvgZqCTIB1JUjMdgj4PAa87OXGOK8lm4aYqLj15GP/cdoDXVu3tkI7bG6zjWqsNJmoI\ngA5rXSeqn4Hk0qgb7NzxVIujJRwj2+e2hCzDeESEwXkB9tmLGK23W71PGpyX3OdzM4bzuRnDAbh+\n1hgKgl5G2nUsCWsikU6YyeLwul22Gy/OiEJLOFojmccD1nWOGev1oUiU7GxP8jq3hm3hKJ5AJNEt\n159LRIKAJRx5WdZxC4M+WsKtGdNxu+OCE0qTrdcL0l0nveCGWWP4xMTiDkvXHgo+j4vLpg/v9esG\nZfsyutkU5UjRl3UcpwMVxpgtxpgw8CxwWdo+NwAPGGNqAYwxiWjuFGCBMSZqjGkGVgKz7X0SoiFA\nFr12zPYer9tK6wOSleMJbjp3HH+5/gyKc/zceM44rv7YyEMWDmi3OlKFoysS6auJgrZQNJ6cyFsj\nUcaV5iDSufgvweA8P/vsyuUNexuseoHS7Iz7lhUG+e4nJyYtl6wuLY6O1ydxLUYWBdu3dWGR+b0u\nYiSEI9whqyoUjUH9Lsgbnqy38HqsJocJ8uw0zkQrk0zpuN0R8LqZPXVIh2McCrkBL6eOOn7dGMqx\nT18Kx3BgZ8rPlfa2VCYCE0XkfRH5wHZtAawAZotIUESKgfOAEYkXicijwF5gMvD7vnoDKedL3vkG\n0ybh0twAHx9XDMD0EQX84nPTOrmFekNSOAYFD7Jnu8Wxdnd7RlSiYrzFrmsYmhfoUshSLY4Ne5sY\nW5zdaeLvisSk3BqJsb2mmXDMEpBUiyN1vxEpQpje/iSBz+0ibv9JtoUiZPtSLI6mOgg3Es8dnrRu\nEpXjCRJxicJsLyId3WY95arTR5Ib8DCmJLOAKooy8JXjHmACcC5wFfAnESkwxswDXgMWAc8Ai4Gk\n094Ycx0wDFgHXJnpwCJyo4iUi0h5VVVVpl16RWISSrc4jjSF2Vafp8E9cHMkKq13p/Q7SjTBS7QV\nn1aWz+iizCI0JC/AvoYQxhg27Gtg0pCe+d+hPcbx+qo9fOK+d1i9yxKv9Mk6sYBUqhB2ZXG4XGIF\nrIHmVqu3V8LieH/ZSgDWteQk1xj32UvHghWfTghSQdB63aEI+KmjCln5s4usAL6iKBnpS+HYRYqV\nAJTZ21KpBF42xkSMMVuxgt0TAIwxdxtjphtjLqQ9EJ7EGBPDcn99MdPJjTEPGWNmGmNmlpQcfvFL\n0uLoIuB6pBicF2DUoGCyqrk7Ute+SLjSEnGOVnsFvN9fdQr3XX5yl+cKR+NU1ray80Brp6B1dwRT\nmj0CyUWUOlkcdquTkSnC0ZXrDMBlC0c8HqUw6E1e9/q9Vp+hJTWBdleV25VcwyOxBgpYbeETQfxD\n4XAsRkU5HuhL4VgCTBCRMSLiA74MvJy2z1+xrA1sl9REYIuIuEWkyN5+EnASME8sxtvbBfgssL4P\n30OSxAR2OPGLnnD77Mn86ZpO9TYZyfJ1bmGe6qoK+tz2mhWZP+bBedbkutBuM30oFkfCbZSIlWSy\nODwuYUhe+0Te3TV0eazfuYhTmO1LHi+Adfx/7gonO8emvre8lDYV375gAi/+68d7/F4URekdfTYL\nGmOiInIzVtcCN/CIMWaNiNwFlBtjXrZ/d5GIrMVyRd1mjKkRkQDwnn3n1wBcbR/PBTwuInlYVsgK\n4Ka+eg+pJCawoL9vXVWleQFKe7F/ca6fxlCU8SU5bK9p6eiq8nb/8Q7JtyyWuXZL7cS6FT3B7RL8\nHhchexLf09CFcPjdFASt9iFulxCLmw69vDof1wNxcBOnMOjD5RICXhdeO4aysSaUbIbndUu7cKQE\ns1Oz3hRFOfL06e2zMeY1rFhF6rY7Up4b4Fb7kbpPG1ZmVfrx4kDmJPw+xm9PRNl97KrqLSU5frZW\nNzPObpvd0BrFGGOtue3r3qAcURhEBN7ZUEWO39OjTK5UsnzupHC0WxwdJ+wzxhRRkGU1YMvxe6hv\njZDj7zpjyeV2Q9QSjkQn4YDXjTdmWVJhvNz1ylpEYHhBVnIBpvwsZ30uinIso9+2HuL3uBDpWAzo\nBEqSa0pYWUANbRFC0Thxc/B4TGlegFdvmcWm/Y0MyQskYwQ9Jeh10yARgj5PclnZ9BjHN84ek3ze\nLhzdWBzudldVoktvlteNz17HIysQYENdK3dcOoUJg3OThYt5GTqqKorSN6hw9JCA1022r3MF9kBT\nbC9+NGJQEI9LaGiNJNuN9KQAbsqwvOQSqb0l4HNzwtA8wtE4m+wV9HzdBPUTLqruXFUu22JxE0+u\nz57ldTMsxw0h+PpZEzggBUlB8mRwVSmK0reocPSQgMfV64Ky/iBhcZTmBsjL8tLQFkmuzNfX4/2X\nc8ZSGPTx54XtnW793VhkCcHoLqvK7bYEwC3t/aLOnVTKafU5UAFfPWu81RzSJpFVpRaHovQfzvK7\nOJhhBVm9jgH0B6eNHsTU4XmUFWZZjQRbo8lq7iO9eEs6V542kotOHJK0DOBgFoc1uaevRpiK2239\nLs/vwm27zu74zBTOGm1nfLk71re0B8f1HkhR+gv9tvWQ2z81OVk/4CTOGFvE32+xup8mLY5wwuLo\nn4+3MLtdOPzduMdy/B58aaucpZOIceT70wQoZvfhcnfsIZUpHVdRlL5FhaOHHA0pnnkBLw2tEVrC\nVgZSf7nWUtf57s7iGFaQxbCDFOa57TqOvE7CEcZaN6Hje/LYVonGOBSl/1DhOIbIy7Kym1oi/eOq\nSjDItjhE2mMOmfjOBRP4ZkqWVSY8tnDkBzIIh9vXaUGcEYVBJg3O5aSyg63qoCjKkUKF4xgiYXH0\nJqvqSJBozOhzu7rNOsvyuQ8qZglXVU56DUosDJ7O/bvyg17mfu+cXo5YUZTDQYPjxxCJGEdLuH+y\nqhIkLI5D6UabjifpqkoToFgY3OqOUhQnoMJxDJEX8NAWiVNvNzrsL1dVIjju62FL9u5ICEeOL5Nw\nHPriSoqiHDlUOI4hEgHi/XYVd79lVdnB8SNjcVjHyu3kqoqoxaEoDkGF4xgi0aIj0Yaj32IcR9BV\n5bUtjmxvmsURDXWq4VAUZWBQ4TiGSKynsXR7LX5PewFdX5Prt1qnp/epOhTUVaUozkeF4xhiXEk2\nAa+L/Y2hfm2PIiId1s44HBIWR066xaGuKkVxDCocxxAet4sThloNC/srvpGgMOg9IhZHSZ7V5Tc3\nUwGgWhyK4ghUOI4xpg6zCuH6u/37x8YWcXJZwWEf57wThgAQTNe9Luo4FEXpf7QA8Bhj6vCBsTju\numzqETmOyy4AxKT1BYuFwd/zpW0VRek71OI4xjjRtjj6q4bjiJPoRRWPddyuripFcQwqHMcYEwfn\n4nM7c+2QHiH2n6RJE46oVo4rilNQV9Uxhs/j4pyJJYwvzRnooRwa3VocGuNQFCegwnEM8udrZg70\nEA4dsYUj3eKIRdRVpSgOQV1VirPo1uJQV5WiOAEVDsVZJC2O9KyqkFociuIQVDgUZ9GlxRHROg5F\ncQgqHIqzEAEkQ4xDXVWK4hRUOBTn4XJ3tDiM0ToORXEQPRIOERknIn77+bki8m0ROfz+EoqSCXF3\ntDhi1sJUKhyK4gx6anG8CMREZDzwEDACeLrPRqUc36RbHLGw9b8Kh6I4gp4KR9wYEwU+D/zeGHMb\nMLTvhqUc14i7Y1aVCoeiOIqeCkdERK4CrgH+bm/TSKXSN7hcaRZHwlWlf3KK4gR6KhzXAWcCdxtj\ntorIGODJvhuWclzTKcYRsv7XdFxFcQQ9ajlijFkLfBtARAqBXGPMPX05MOU4plOMQ4PjiuIkeppV\n9Y6I5InIIGAZ8CcR+U0PXjdbRDaISIWI3N7FPleIyFoRWSMiT6dsv0dEVtuPK1O2P2Ufc7WIPCIi\n6r841uhkcSRiHPpRK4oT6KmrKt8Y0wB8AXjCGHMG8MnuXiAibuAB4FPAFOAqEZmSts8E4EfAWcaY\nE4Hv2tsvAU4BpgNnAD8QkTz7ZU8Bk4FpQBZwfQ/fg3K04HJDPCU4HrVdVWpxKIoj6KlweERkKHAF\n7cHxg3E6UGGM2WKMCQPPApel7XMD8IAxphbAGLPf3j4FWGCMiRpjmoGVwGx7n9eMDfBPoKyH41GO\nFrqs49AYh6I4gZ4Kx13AXGCzMWaJiIwFNh3kNcOBnSk/V9rbUpkITBSR90XkAxGZbW9fAcwWkaCI\nFAPnYdWOJLFdVF8D3sh0chG5UUTKRaS8qqqqB29RcQydsqrUVaUoTqKnwfHngedTft4CfPEInX8C\ncC6W5bBARKYZY+aJyGnAIqAKWAykNS/if7Gskve6GPNDWMWKzJw50xyBsSr9RZcxDnVVKYoT6Glw\nvExEXhKR/fbjRRE5mItoFx2thDJ7WyqVwMvGmIgxZiuwEUtIMMbcbYyZboy5EBD7d4nx/AwoAW7t\nyfiVowytHFcUR9NTV9WjwMvAMPvxir2tO5YAE0RkjIj4gC/bx0jlr1jWBrZLaiKwRUTcIlJkbz8J\nOAmYZ/98PXAxcJUx6Ys2KMcEXVkcHhUORXECPRWOEmPMo3awOmqMeQzrjr9L7BYlN2PFRtYBc4wx\na0TkLhH5rL3bXKBGRNYC84HbjDE1WFXp79nbHwKuto8H8CAwGFgsIstF5I6ev13lqCA9q0otDkVx\nFD1dc7xGRK4GnrF/vgqoOdiLjDGvAa+lbbsj5bnBcjfdmrZPG1ZmVaZj6jrpxzri6iKrSoPjiuIE\nempxfAMrFXcvsAf4EnBtH41JOd5Jj3FoHYeiOIoeCYcxZrsx5rPGmBJjTKkx5nMcmawqRelMl1lV\nWsehKE7gcFYA1IwmpW/osleVuqoUxQkcjnDIERuFoqSi63EoiqM5HOHQojqlb9A6DkVxNN1mKIlI\nI5kFQrAaDCrKkUdcYMLtP8fC1ja3JtQpihPo9ptojMntr4EoSpJMFodaG4riGA7HVaUofUN6VlVU\nhUNRnIQKh+I8MlocmlGlKE5BhUNxHpmyqrSGQ1EcgwqH4jw6rccRUYtDURyECofiPDpVjoc0xqEo\nDkKFQ3EemSrHPeqqUhSnoMKhOI9MvarUVaUojkGFQ3EemdbjUFeVojgGFQ7FeWgdh6I4GhUOxXl0\nyqpS4VAUJ6HCoTiPjDEOFQ5FcQoqHIrzyJRVpQ0OFcUxqHAozkO0yaGiOBkVDsV5uNJcVfEIuDQd\nV1GcggqH4jwkPTge1ToORXEQKhyK80i3OLQAUFEchQqH4jzSYxzqqlIUR6HCoTiPThaHuqoUxUmo\ncCjOQ9zW/4m2I+qqUhRHocKhOA+XLRwmBsaoq0pRHIZWVSnOQ+z7mXgMEOu5WhyK4hhUOBTnkWpx\nxGx3lQqHojgGFQ7FeSRjHDHAWM/VVaUojkGFQ3EeqRZHXC0ORXEaGhxXnEdqVlUsbD1X4VAUx9Cn\nwiEis0Vkg4hUiMjtXexzhYisFZE1IvJ0yvZ7RGS1/bgyZfvN9vGMiBT35fiVAaKDxRGxt6lwKIpT\n6DNXlYi4gQeAC4FKYImIvGyMWZuyzwTgR8BZxphaESm1t18CnAJMB/zAOyLyujGmAXgf+DvwTl+N\nXRlgUrOqYrZwqMWhKI6hLy2O04EKY8wWY0wYeBa4LG2fG4AHjDG1AMaY/fb2KcACY0zUGNMMrARm\n2/t8ZIzZ1ofjVgaaDhZH1HquwqEojqEvhWM4sDPl50p7WyoTgYki8r6IfCAis+3tK4DZIhK03VHn\nASN6c3IRuVFEykWkvKqq6hDfgjIgpGZVJWIc6qpSFMcw0FlVHmACcC5QBiwQkWnGmHkichqwCKgC\nFgOxLo+SAWPMQ8BDADNnzjRHctBKH9OhjkNdVYriNPrS4thFRyuhzN6WSiXwsjEmYozZCmzEEhKM\nMXcbY6YbYy7EKh/e2IdjVZxEalaVuqoUxXH0pXAsASaIyBgR8QFfBl5O2+evWNYGtktqIrBFRNwi\nUmRvPwk4CZjXh2NVnEQHi0NdVYriNPpMOIwxUeBmYC6wDphjjFkjIneJyGft3eYCNSKyFpgP3GaM\nqQG8wHv29oeAq+3jISLfFpFKLAtmpYj8ua/egzJAuFJjHOqqUhSn0acxDmPMa8BradvuSHlugFvt\nR+o+bViZVZmOeT9w/xEfrOIcJFNWlW/gxqMoSge0clxxHq5MWVUDncehKEoCFQ7FeYhmVSmKk1Hh\nUJyHK1NWlbqqFMUpqHAoziPRcsSoq0pRnIgKh+I8NKtKURyNCofiPCRDd1x1VSmKY1DhUJxHJotD\nXVWK4hhUOBTnoVlViuJoVDgU59Ehq0oXclIUp6HCoTiPDllV2uRQUZyGCofiPNIrx8XVvk1RlAFH\nhUNxHulZVeqmUhRHocKhOI8OFkdUU3EVxWGocCjOI2lxxC1XlVtTcRXFSahwKM4j1eJQV5WiOA4V\nDsV5pGdVqatKURyFCofiPNKzqtRVpSiOQoVDcR6aVaUojkaFQ3Ee6b2q1FWlKI5ChUNxHh2yqiLq\nqlIUh6HCoTgPzapSFEejwqE4jw5ZVeqqUhSnocKhOI9OMQ51VSmKk1DhUJyHZlUpiqNR4VCch2ZV\nKYqjUeFQnEdqVlU8qq4qRXEYKhyK80ivHFdXlaI4ChUOxXmIAKJZVYriUFQ4FGfictt1HOqqUhSn\nocKhOBNx2xaHuqoUxWmocCjOJGFxxCLgVuFQFCfRp8IhIrNFZIOIVIjI7V3sc4WIrBWRNSLydMr2\ne0Rktf24MmX7GBH50D7mcyKiDvBjEXGnZFXpR6woTqLPhENE3MADwKeAKcBVIjIlbZ8JwI+As4wx\nJwLftbdfApwCTAfOAH4gInn2y+4BfmuMGQ/UAt/sq/egDCAuV0pWlcY4FMVJ9KXFcTpQYYzZYowJ\nA88Cl6XtcwPwgDGmFsAYs9/ePgVYYIyJGmOagZXAbBER4HzgBXu/x4HP9eF7UAaKZIxDXVWK4jT6\nUjiGAztTfq60t6UyEZgoIu+LyAciMtvevgJLKIIiUgycB4wAioA6Y0y0m2MCICI3iki5iJRXVVUd\nobek9Bsut2VtYNRVR6eneAAACLRJREFUpSgOY6B9AB5gAnAuUAYsEJFpxph5InIasAioAhYDsd4c\n2BjzEPAQwMyZM82RHLTSD4gboiHrubqqFMVR9KXFsQvLSkhQZm9LpRJ42RgTMcZsBTZiCQnGmLuN\nMdONMRcCYv+uBigQEU83x1SOBVxuiLRaz9VVpSiOoi+FYwkwwc6C8gFfBl5O2+evWNYGtktqIrBF\nRNwiUmRvPwk4CZhnjDHAfOBL9uuvAf7Wh+9BGSiyCqHJDnmpq0pRHEWfCYcdh7gZmAusA+YYY9aI\nyF0i8ll7t7lAjYisxRKE24wxNYAXeM/e/hBwdUpc44fArSJSgRXzeLiv3oMygOQOhdpt1nN1VSmK\no+jTb6Qx5jXgtbRtd6Q8N8Ct9iN1nzaszKpMx9yClbGlHMvkDYOKN63n6qpSFEehleOKM8lLSZZT\nV5WiOAoVDsWZ5A1rf66uKkVxFCocijNJFQ51VSmKo1DhUJyJuqoUxbGocCjOpIOrSi0ORXESKhyK\nMwnkgS/Heq4LOSmKo1DhUJxLwupQV5WiOAoVDsW5JIRDXVWK4ihUOBTnkgiQq6tKURyFCofiXNRV\npSiORIVDcS7qqlIUR6I+AMW5TP4M1G6HonEDPRJFUVJQ4VCcS04JXPifAz0KRVHSUFeVoiiK0itU\nOBRFUZReocKhKIqi9AoVDkVRFKVXqHAoiqIovUKFQ1EURekVKhyKoihKr1DhUBRFUXqFGGMGegx9\njohUAdsP8eXFQPURHM6RwqnjAueOTcfVO3RcvcepYzvUcY0yxpSkbzwuhONwEJFyY8zMgR5HOk4d\nFzh3bDqu3qHj6j1OHduRHpe6qhRFUZReocKhKIqi9AoVjoPz0EAPoAucOi5w7th0XL1Dx9V7nDq2\nIzoujXEoiqIovUItDkVRFKVXqHAoiqIovUKFoxtEZLaIbBCRChG5fQDHMUJE5ovIWhFZIyLfsbff\nKSK7RGS5/fj0AIxtm4isss9fbm8bJCJvisgm+//Cfh7TpJRrslxEGkTkuwN1vUTkERHZLyKrU7Zl\nvEZicb/9N7dSRE7p53HdJyLr7XO/JCIF9vbRItKacu0e7OdxdfnZiciP7Ou1QUQu7udxPZcypm0i\nstze3p/Xq6v5oe/+xowx+sjwANzAZmAs4ANWAFMGaCxDgVPs57nARmAKcCfwgwG+TtuA4rRt9wK3\n289vB+4Z4M9xLzBqoK4XcA5wCrD6YNcI+DTwOiDAx4AP+3lcFwEe+/k9KeManbrfAFyvjJ+d/T1Y\nAfiBMfZ31t1f40r7/a+BOwbgenU1P/TZ35haHF1zOlBhjNlijAkDzwKXDcRAjDF7jDHL7OeNwDpg\n+ECMpYdcBjxuP38c+NwAjuUCYLMx5lA7Bxw2xpgFwIG0zV1do8uAJ4zFB0CBiAztr3EZY+YZY6L2\njx8AZX1x7t6OqxsuA541xoSMMVuBCqzvbr+OS0QEuAJ4pi/O3R3dzA999jemwtE1w4GdKT9X4oDJ\nWkRGAzOAD+1NN9vm5iP97RKyMcA8EVkqIjfa2wYbY/bYz/cCgwdgXAm+TMcv80BfrwRdXSMn/d19\nA+vONMEYEflIRN4VkVkDMJ5Mn93/b+9eQuSoojCO/z8mUYZEg0YRIYYkOm5EjZKFSHQhLhzRgLpI\nQsAo2SSIKIK6mK0rFyLRoBhERSKIqDgrUUcQQSFiyBMfkeDCME4eYESUEMfj4p6Wmsl0sKCraqLf\nD5quPtPTnD5V1K17q/rWfKnXbcBURByuxFqv16z9Q2PbmBuO84ikxcC7wOMR8SvwEnA1sBqYpHSV\n27Y2Im4GRoFHJN1e/WOUvnEn13xLugBYB7yToflQr7N0WaN+JI0BfwK7MjQJLI+Im4AngLckXdxi\nSvNy3VVsZOYBSuv1mmP/8I9Bb2NuOPo7ClxVeb0sY52QtJCyUeyKiPcAImIqIqYj4i9gJw110c8l\nIo7m8zHg/cxhqtf1zedjbeeVRoE9ETGVOXZer4p+Nep8u5P0EHAPsCl3OORQ0Mlc/ppyLuHatnI6\nx7qbD/VaANwPvN2LtV2vufYPNLiNueHo7ytgRNLKPHLdAIx3kUiOn74KfBMRz1Xi1XHJ+4CDs/+3\n4bwWSbqot0w5sXqQUqfN+bbNwAdt5lUx4yiw63rN0q9G48CDeeXLLcCpynBD4yTdBTwFrIuI3yvx\nyyUN5fIqYAQ40mJe/dbdOLBB0oWSVmZeu9vKK90JfBsRP/UCbdar3/6BJrexNs76n68PytUH31OO\nFsY6zGMtpZu5H9ibj7uBN4EDGR8Hrmw5r1WUK1r2AYd6NQKWAhPAYeAT4NIOarYIOAksqcQ6qRel\n8ZoEzlDGk7f0qxHlSpcduc0dANa0nNcPlPHv3nb2cr73gVzHe4E9wL0t59V33QFjWa/vgNE288r4\n68DWWe9ts1799g+NbWOecsTMzGrxUJWZmdXihsPMzGpxw2FmZrW44TAzs1rccJiZWS1uOMwGQNK0\nZs7IO7DZlHOm1S5/c2I2w4KuEzD7j/gjIlZ3nYRZG9zjMGtQ3qPhWZV7luyWdE3GV0j6NCftm5C0\nPONXqNwHY18+bs2PGpK0M++38JGk4c6+lP3vueEwG4zhWUNV6yt/OxUR1wMvAs9n7AXgjYi4gTKR\n4PaMbwc+i4gbKfd+OJTxEWBHRFwH/EL5ZbJZJ/zLcbMBkPRbRCyeI/4jcEdEHMmJ6H6OiKWSTlCm\nzTiT8cmIuEzScWBZRJyufMYK4OOIGMnXTwMLI+KZ5r+Z2dnc4zBrXvRZruN0ZXkan5+0DrnhMGve\n+srzl7n8BWXGZYBNwOe5PAFsA5A0JGlJW0ma/Vs+ajEbjGFJeyuvP4yI3iW5l0jaT+k1bMzYo8Br\nkp4EjgMPZ/wx4BVJWyg9i22UGVnN5g2f4zBrUJ7jWBMRJ7rOxWxQPFRlZma1uMdhZma1uMdhZma1\nuOEwM7Na3HCYmVktbjjMzKwWNxxmZlbL39C9oIAjpKw/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW6PcceTwDGY",
        "colab_type": "code",
        "outputId": "f6c8739f-67fb-4b9b-c5df-5abc2d17c732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "pYtest = np.argmax(model.predict(Xtest), axis=1)\n",
        "CM = confusion_matrix(y_pred=pYtest, y_true=Ytest.argmax(axis=1))\n",
        "print(CM)\n",
        "\n",
        "print(classification_report(y_pred=pYtest, y_true=Ytest.argmax(axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[82  0]\n",
            " [82  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67        82\n",
            "           1       0.00      0.00      0.00        82\n",
            "\n",
            "    accuracy                           0.50       164\n",
            "   macro avg       0.25      0.50      0.33       164\n",
            "weighted avg       0.25      0.50      0.33       164\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd2JouGQqWxd",
        "colab_type": "code",
        "outputId": "bec0370b-ae61-43ef-f3ee-2c4325bc493e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# for yp in Yp:\n",
        "#     print('{:0.3f} {:0.3f}'.format(yp[0], yp[1]))\n",
        "\n",
        "\n",
        "# A = (Yp.argmax(axis=1) != Ytest.argmax(axis=1))\n",
        "# print(A)\n",
        "# for a in A:\n",
        "#     if a[0] == a[1]:\n",
        "#         C += 1\n",
        "#end-for\n",
        "\n",
        "C = 0\n",
        "for yp, y, prediction, actual in zip(Yp, Ytest, Yp.argmax(axis=1), Ytest.argmax(axis=1)):\n",
        "    print('{:0.2f} {:0.2f}'.format(yp[0], yp[1]), end='|')\n",
        "    print('{} {}'.format(y[0], y[1]), end='')\n",
        "    if prediction == actual:\n",
        "        print(' ✔')\n",
        "        C = C + 1\n",
        "    else:\n",
        "        print(' ✘')\n",
        "#end-for\n",
        "\n",
        "print(C/164)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|1 0 ✔\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.50 0.50|0 1 ✘\n",
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtk2paqysXn9",
        "colab_type": "code",
        "outputId": "f885435c-d9b6-42d2-9921-99f723f6e4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "help(EarlyStopping.__init__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function __init__ in module tensorflow.python.keras.callbacks:\n",
            "\n",
            "__init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
            "    Initialize self.  See help(type(self)) for accurate signature.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_e5dtrWedn",
        "colab_type": "code",
        "outputId": "edf3caf3-4f68-403b-8783-6b82a3de1541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "help(BatchNormalization.__init__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function __init__ in module tensorflow.python.keras.layers.normalization:\n",
            "\n",
            "__init__(self, axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqVsgw1KJA5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Re7qajUGYP",
        "colab_type": "text"
      },
      "source": [
        "### RNN Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39RW-9g7UKLt",
        "colab_type": "code",
        "outputId": "4fe07288-9ffa-4adf-8ff5-6db05d103580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(Xtrain.shape)\n",
        "print(Xtest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 220, 1)\n",
            "(164, 220, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4LUG-0BUQjf",
        "colab_type": "code",
        "outputId": "de297007-f067-469f-ccd2-ea01664b6e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "def Network():\n",
        "    i = Input(shape=(220, 1), name='Input')\n",
        "    # x = SimpleRNN(units=32, activation='tanh', name='Simple-RNN')(i)\n",
        "    x = LSTM(units=128, activation='tanh', name='lstm-1')(i)\n",
        "    x = Dense(units=2, activation='softmax', name='Dense')(x)\n",
        "\n",
        "    return Model(inputs=[i], outputs=[x])\n",
        "#end-def\n",
        "\n",
        "model = Network()\n",
        "\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False, expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           [(None, 220, 1)]          0         \n",
            "_________________________________________________________________\n",
            "lstm-1 (LSTM)                (None, 128)               66560     \n",
            "_________________________________________________________________\n",
            "Dense (Dense)                (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 66,818\n",
            "Trainable params: 66,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEnCAYAAADIAdSUAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT2giMQcUlx8",
        "colab_type": "code",
        "outputId": "e7450b41-8941-4bce-fcdb-6b36655a943e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Watch Tower:\n",
        "# monitor = EarlyStopping(monitor='val_loss', verbose=1, patience=3)\n",
        "\n",
        "# Train the model:\n",
        "result = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), epochs=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 500 samples, validate on 164 samples\n",
            "Epoch 1/600\n",
            "500/500 [==============================] - 1s 3ms/sample - loss: 0.6975 - accuracy: 0.4940 - val_loss: 0.6947 - val_accuracy: 0.4939\n",
            "Epoch 2/600\n",
            "500/500 [==============================] - 0s 547us/sample - loss: 0.6922 - accuracy: 0.5120 - val_loss: 0.6945 - val_accuracy: 0.5366\n",
            "Epoch 3/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6915 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5427\n",
            "Epoch 4/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.6899 - accuracy: 0.5140 - val_loss: 0.6948 - val_accuracy: 0.4817\n",
            "Epoch 5/600\n",
            "500/500 [==============================] - 0s 445us/sample - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6936 - val_accuracy: 0.5305\n",
            "Epoch 6/600\n",
            "500/500 [==============================] - 0s 479us/sample - loss: 0.6907 - accuracy: 0.5060 - val_loss: 0.6929 - val_accuracy: 0.4878\n",
            "Epoch 7/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6853 - accuracy: 0.5440 - val_loss: 0.6928 - val_accuracy: 0.5122\n",
            "Epoch 8/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.6836 - accuracy: 0.5600 - val_loss: 0.6891 - val_accuracy: 0.4695\n",
            "Epoch 9/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.6817 - accuracy: 0.5400 - val_loss: 0.6937 - val_accuracy: 0.5183\n",
            "Epoch 10/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6705 - accuracy: 0.5520 - val_loss: 0.6999 - val_accuracy: 0.5183\n",
            "Epoch 11/600\n",
            "500/500 [==============================] - 0s 398us/sample - loss: 0.7003 - accuracy: 0.5320 - val_loss: 0.6934 - val_accuracy: 0.5244\n",
            "Epoch 12/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.6911 - accuracy: 0.5040 - val_loss: 0.6912 - val_accuracy: 0.5122\n",
            "Epoch 13/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.6909 - val_accuracy: 0.4817\n",
            "Epoch 14/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.6810 - accuracy: 0.5420 - val_loss: 0.6907 - val_accuracy: 0.5122\n",
            "Epoch 15/600\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.6793 - accuracy: 0.5600 - val_loss: 0.6909 - val_accuracy: 0.5183\n",
            "Epoch 16/600\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.6760 - accuracy: 0.5680 - val_loss: 0.6896 - val_accuracy: 0.5122\n",
            "Epoch 17/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6760 - accuracy: 0.5520 - val_loss: 0.6921 - val_accuracy: 0.5488\n",
            "Epoch 18/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6746 - accuracy: 0.5560 - val_loss: 0.6882 - val_accuracy: 0.5183\n",
            "Epoch 19/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.6730 - accuracy: 0.5640 - val_loss: 0.6830 - val_accuracy: 0.5061\n",
            "Epoch 20/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.6643 - accuracy: 0.6060 - val_loss: 0.6795 - val_accuracy: 0.5854\n",
            "Epoch 21/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6620 - accuracy: 0.6060 - val_loss: 0.7027 - val_accuracy: 0.5427\n",
            "Epoch 22/600\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.6749 - accuracy: 0.5780 - val_loss: 0.6796 - val_accuracy: 0.5915\n",
            "Epoch 23/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.6623 - accuracy: 0.6040 - val_loss: 0.6756 - val_accuracy: 0.5793\n",
            "Epoch 24/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.6486 - accuracy: 0.6240 - val_loss: 0.6682 - val_accuracy: 0.6037\n",
            "Epoch 25/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.6384 - accuracy: 0.6400 - val_loss: 0.6624 - val_accuracy: 0.5915\n",
            "Epoch 26/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6394 - accuracy: 0.5940 - val_loss: 0.6672 - val_accuracy: 0.6220\n",
            "Epoch 27/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.6270 - accuracy: 0.6300 - val_loss: 0.6554 - val_accuracy: 0.6220\n",
            "Epoch 28/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.6290 - accuracy: 0.6360 - val_loss: 0.6550 - val_accuracy: 0.6159\n",
            "Epoch 29/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.6311 - accuracy: 0.6560 - val_loss: 0.6361 - val_accuracy: 0.6524\n",
            "Epoch 30/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.6073 - accuracy: 0.6720 - val_loss: 0.6250 - val_accuracy: 0.6098\n",
            "Epoch 31/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6171 - accuracy: 0.6340 - val_loss: 0.6421 - val_accuracy: 0.6037\n",
            "Epoch 32/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6248 - accuracy: 0.6400 - val_loss: 0.6605 - val_accuracy: 0.5427\n",
            "Epoch 33/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6135 - accuracy: 0.6380 - val_loss: 0.6445 - val_accuracy: 0.5854\n",
            "Epoch 34/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.6006 - accuracy: 0.6780 - val_loss: 0.6216 - val_accuracy: 0.6524\n",
            "Epoch 35/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.6105 - accuracy: 0.6520 - val_loss: 0.6817 - val_accuracy: 0.5915\n",
            "Epoch 36/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6526 - accuracy: 0.6040 - val_loss: 0.6618 - val_accuracy: 0.5854\n",
            "Epoch 37/600\n",
            "500/500 [==============================] - 0s 398us/sample - loss: 0.6666 - accuracy: 0.5720 - val_loss: 0.6626 - val_accuracy: 0.6037\n",
            "Epoch 38/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6498 - accuracy: 0.5980 - val_loss: 0.6478 - val_accuracy: 0.6341\n",
            "Epoch 39/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.6181 - accuracy: 0.6760 - val_loss: 0.6263 - val_accuracy: 0.6463\n",
            "Epoch 40/600\n",
            "500/500 [==============================] - 0s 398us/sample - loss: 0.6051 - accuracy: 0.6640 - val_loss: 0.6331 - val_accuracy: 0.6402\n",
            "Epoch 41/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6053 - accuracy: 0.6640 - val_loss: 0.6636 - val_accuracy: 0.6037\n",
            "Epoch 42/600\n",
            "500/500 [==============================] - 0s 447us/sample - loss: 0.6053 - accuracy: 0.6480 - val_loss: 0.6588 - val_accuracy: 0.5854\n",
            "Epoch 43/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.5892 - accuracy: 0.7080 - val_loss: 0.6867 - val_accuracy: 0.6159\n",
            "Epoch 44/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.6504 - accuracy: 0.6380 - val_loss: 0.6773 - val_accuracy: 0.6341\n",
            "Epoch 45/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.6254 - accuracy: 0.6340 - val_loss: 0.8001 - val_accuracy: 0.5122\n",
            "Epoch 46/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.6496 - accuracy: 0.6060 - val_loss: 0.6589 - val_accuracy: 0.6463\n",
            "Epoch 47/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.6258 - accuracy: 0.6260 - val_loss: 0.6449 - val_accuracy: 0.6585\n",
            "Epoch 48/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.6021 - accuracy: 0.6500 - val_loss: 0.6366 - val_accuracy: 0.6524\n",
            "Epoch 49/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.5905 - accuracy: 0.6760 - val_loss: 0.6252 - val_accuracy: 0.6463\n",
            "Epoch 50/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.5761 - accuracy: 0.6980 - val_loss: 0.6375 - val_accuracy: 0.6524\n",
            "Epoch 51/600\n",
            "500/500 [==============================] - 0s 398us/sample - loss: 0.5824 - accuracy: 0.6680 - val_loss: 0.6405 - val_accuracy: 0.6341\n",
            "Epoch 52/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.5728 - accuracy: 0.6980 - val_loss: 0.6580 - val_accuracy: 0.6524\n",
            "Epoch 53/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.6035 - accuracy: 0.6740 - val_loss: 0.6582 - val_accuracy: 0.6341\n",
            "Epoch 54/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.5881 - accuracy: 0.6980 - val_loss: 0.6478 - val_accuracy: 0.6280\n",
            "Epoch 55/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.5551 - accuracy: 0.7220 - val_loss: 0.6985 - val_accuracy: 0.5976\n",
            "Epoch 56/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.5549 - accuracy: 0.7160 - val_loss: 0.6584 - val_accuracy: 0.6402\n",
            "Epoch 57/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.5526 - accuracy: 0.7320 - val_loss: 0.6647 - val_accuracy: 0.6402\n",
            "Epoch 58/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.5547 - accuracy: 0.7300 - val_loss: 0.6746 - val_accuracy: 0.6098\n",
            "Epoch 59/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.5494 - accuracy: 0.7260 - val_loss: 0.6420 - val_accuracy: 0.6280\n",
            "Epoch 60/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.5372 - accuracy: 0.7180 - val_loss: 0.6990 - val_accuracy: 0.6402\n",
            "Epoch 61/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.5273 - accuracy: 0.7480 - val_loss: 0.7768 - val_accuracy: 0.5671\n",
            "Epoch 62/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.5707 - accuracy: 0.6840 - val_loss: 0.6753 - val_accuracy: 0.6280\n",
            "Epoch 63/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.5908 - accuracy: 0.6960 - val_loss: 0.6718 - val_accuracy: 0.6341\n",
            "Epoch 64/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.5597 - accuracy: 0.7140 - val_loss: 0.6492 - val_accuracy: 0.5976\n",
            "Epoch 65/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.5159 - accuracy: 0.7720 - val_loss: 0.6911 - val_accuracy: 0.5915\n",
            "Epoch 66/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.5080 - accuracy: 0.7760 - val_loss: 0.6555 - val_accuracy: 0.6585\n",
            "Epoch 67/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.5025 - accuracy: 0.7460 - val_loss: 0.6587 - val_accuracy: 0.6585\n",
            "Epoch 68/600\n",
            "500/500 [==============================] - 0s 447us/sample - loss: 0.4748 - accuracy: 0.7760 - val_loss: 0.6891 - val_accuracy: 0.6341\n",
            "Epoch 69/600\n",
            "500/500 [==============================] - 0s 493us/sample - loss: 0.4528 - accuracy: 0.7860 - val_loss: 0.7192 - val_accuracy: 0.6159\n",
            "Epoch 70/600\n",
            "500/500 [==============================] - 0s 501us/sample - loss: 0.4931 - accuracy: 0.7680 - val_loss: 0.7003 - val_accuracy: 0.6524\n",
            "Epoch 71/600\n",
            "500/500 [==============================] - 0s 481us/sample - loss: 0.4891 - accuracy: 0.7640 - val_loss: 0.6978 - val_accuracy: 0.6524\n",
            "Epoch 72/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.4430 - accuracy: 0.7860 - val_loss: 0.6802 - val_accuracy: 0.6463\n",
            "Epoch 73/600\n",
            "500/500 [==============================] - 0s 449us/sample - loss: 0.5531 - accuracy: 0.7360 - val_loss: 0.6782 - val_accuracy: 0.6890\n",
            "Epoch 74/600\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.5577 - accuracy: 0.7120 - val_loss: 0.6757 - val_accuracy: 0.6524\n",
            "Epoch 75/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.5117 - accuracy: 0.7460 - val_loss: 0.6741 - val_accuracy: 0.7073\n",
            "Epoch 76/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.4800 - accuracy: 0.7720 - val_loss: 0.7274 - val_accuracy: 0.6159\n",
            "Epoch 77/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.4612 - accuracy: 0.7860 - val_loss: 0.7067 - val_accuracy: 0.6829\n",
            "Epoch 78/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.4429 - accuracy: 0.8080 - val_loss: 0.7201 - val_accuracy: 0.6768\n",
            "Epoch 79/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.4757 - accuracy: 0.7900 - val_loss: 0.7235 - val_accuracy: 0.6402\n",
            "Epoch 80/600\n",
            "500/500 [==============================] - 0s 397us/sample - loss: 0.4200 - accuracy: 0.8140 - val_loss: 0.7133 - val_accuracy: 0.6402\n",
            "Epoch 81/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.4286 - accuracy: 0.7820 - val_loss: 0.8103 - val_accuracy: 0.6037\n",
            "Epoch 82/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.4884 - accuracy: 0.7720 - val_loss: 0.7276 - val_accuracy: 0.6402\n",
            "Epoch 83/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.4669 - accuracy: 0.7760 - val_loss: 0.7162 - val_accuracy: 0.6463\n",
            "Epoch 84/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.4541 - accuracy: 0.7900 - val_loss: 0.7679 - val_accuracy: 0.6280\n",
            "Epoch 85/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.4550 - accuracy: 0.7940 - val_loss: 0.7684 - val_accuracy: 0.6280\n",
            "Epoch 86/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.4786 - accuracy: 0.7520 - val_loss: 0.7900 - val_accuracy: 0.6220\n",
            "Epoch 87/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.5112 - accuracy: 0.7700 - val_loss: 0.7352 - val_accuracy: 0.6463\n",
            "Epoch 88/600\n",
            "500/500 [==============================] - 0s 465us/sample - loss: 0.5959 - accuracy: 0.6760 - val_loss: 0.7090 - val_accuracy: 0.6159\n",
            "Epoch 89/600\n",
            "500/500 [==============================] - 0s 491us/sample - loss: 0.5507 - accuracy: 0.7080 - val_loss: 0.6710 - val_accuracy: 0.6524\n",
            "Epoch 90/600\n",
            "500/500 [==============================] - 0s 482us/sample - loss: 0.5056 - accuracy: 0.7680 - val_loss: 0.6771 - val_accuracy: 0.6524\n",
            "Epoch 91/600\n",
            "500/500 [==============================] - 0s 452us/sample - loss: 0.4554 - accuracy: 0.7920 - val_loss: 0.7119 - val_accuracy: 0.6463\n",
            "Epoch 92/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.4428 - accuracy: 0.7900 - val_loss: 0.7439 - val_accuracy: 0.6463\n",
            "Epoch 93/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.3947 - accuracy: 0.8320 - val_loss: 0.7579 - val_accuracy: 0.6524\n",
            "Epoch 94/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.4119 - accuracy: 0.8140 - val_loss: 0.7372 - val_accuracy: 0.6463\n",
            "Epoch 95/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.4203 - accuracy: 0.8280 - val_loss: 0.7329 - val_accuracy: 0.6707\n",
            "Epoch 96/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.3635 - accuracy: 0.8520 - val_loss: 0.8008 - val_accuracy: 0.6402\n",
            "Epoch 97/600\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.3869 - accuracy: 0.8360 - val_loss: 0.7770 - val_accuracy: 0.6402\n",
            "Epoch 98/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.4356 - accuracy: 0.8180 - val_loss: 0.7587 - val_accuracy: 0.6585\n",
            "Epoch 99/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.3924 - accuracy: 0.8380 - val_loss: 0.7951 - val_accuracy: 0.6829\n",
            "Epoch 100/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.3529 - accuracy: 0.8440 - val_loss: 0.8468 - val_accuracy: 0.6463\n",
            "Epoch 101/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.3228 - accuracy: 0.8680 - val_loss: 0.8029 - val_accuracy: 0.6951\n",
            "Epoch 102/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.3249 - accuracy: 0.8800 - val_loss: 0.8463 - val_accuracy: 0.6524\n",
            "Epoch 103/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.3419 - accuracy: 0.8480 - val_loss: 0.8564 - val_accuracy: 0.6402\n",
            "Epoch 104/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.3621 - accuracy: 0.8480 - val_loss: 0.8610 - val_accuracy: 0.6280\n",
            "Epoch 105/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.3594 - accuracy: 0.8580 - val_loss: 0.8395 - val_accuracy: 0.6707\n",
            "Epoch 106/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.4730 - accuracy: 0.7780 - val_loss: 0.8630 - val_accuracy: 0.6341\n",
            "Epoch 107/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.3902 - accuracy: 0.8340 - val_loss: 0.8579 - val_accuracy: 0.6341\n",
            "Epoch 108/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.3163 - accuracy: 0.8680 - val_loss: 0.8930 - val_accuracy: 0.6341\n",
            "Epoch 109/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.2848 - accuracy: 0.8920 - val_loss: 0.8762 - val_accuracy: 0.6524\n",
            "Epoch 110/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.2918 - accuracy: 0.8820 - val_loss: 0.9393 - val_accuracy: 0.6768\n",
            "Epoch 111/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.2552 - accuracy: 0.9080 - val_loss: 0.9825 - val_accuracy: 0.6280\n",
            "Epoch 112/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.2548 - accuracy: 0.9040 - val_loss: 0.9355 - val_accuracy: 0.6402\n",
            "Epoch 113/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.4413 - accuracy: 0.7980 - val_loss: 0.9378 - val_accuracy: 0.5732\n",
            "Epoch 114/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.5218 - accuracy: 0.7680 - val_loss: 0.8929 - val_accuracy: 0.5610\n",
            "Epoch 115/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.4525 - accuracy: 0.7720 - val_loss: 0.8100 - val_accuracy: 0.6220\n",
            "Epoch 116/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.3508 - accuracy: 0.8500 - val_loss: 0.8473 - val_accuracy: 0.6037\n",
            "Epoch 117/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.3421 - accuracy: 0.8500 - val_loss: 0.8527 - val_accuracy: 0.6280\n",
            "Epoch 118/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.3012 - accuracy: 0.8740 - val_loss: 0.9106 - val_accuracy: 0.6037\n",
            "Epoch 119/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.2756 - accuracy: 0.8940 - val_loss: 0.9257 - val_accuracy: 0.6341\n",
            "Epoch 120/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.2484 - accuracy: 0.8880 - val_loss: 0.9190 - val_accuracy: 0.6646\n",
            "Epoch 121/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.2636 - accuracy: 0.8780 - val_loss: 0.9908 - val_accuracy: 0.6463\n",
            "Epoch 122/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.2933 - accuracy: 0.8560 - val_loss: 0.9841 - val_accuracy: 0.6098\n",
            "Epoch 123/600\n",
            "500/500 [==============================] - 0s 466us/sample - loss: 0.3269 - accuracy: 0.8500 - val_loss: 1.0263 - val_accuracy: 0.6341\n",
            "Epoch 124/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.2662 - accuracy: 0.8900 - val_loss: 1.0323 - val_accuracy: 0.6037\n",
            "Epoch 125/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.2625 - accuracy: 0.8720 - val_loss: 1.1089 - val_accuracy: 0.6159\n",
            "Epoch 126/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.2740 - accuracy: 0.8840 - val_loss: 1.0472 - val_accuracy: 0.6341\n",
            "Epoch 127/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.2483 - accuracy: 0.8980 - val_loss: 0.9744 - val_accuracy: 0.6707\n",
            "Epoch 128/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.2460 - accuracy: 0.9040 - val_loss: 0.9740 - val_accuracy: 0.6524\n",
            "Epoch 129/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.2536 - accuracy: 0.8960 - val_loss: 0.9561 - val_accuracy: 0.6768\n",
            "Epoch 130/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.2480 - accuracy: 0.9020 - val_loss: 1.1438 - val_accuracy: 0.6646\n",
            "Epoch 131/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.2651 - accuracy: 0.8980 - val_loss: 0.9888 - val_accuracy: 0.6829\n",
            "Epoch 132/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.3535 - accuracy: 0.8560 - val_loss: 1.0601 - val_accuracy: 0.6280\n",
            "Epoch 133/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.2957 - accuracy: 0.8720 - val_loss: 0.9439 - val_accuracy: 0.6524\n",
            "Epoch 134/600\n",
            "500/500 [==============================] - 0s 441us/sample - loss: 0.3220 - accuracy: 0.8680 - val_loss: 0.9834 - val_accuracy: 0.6341\n",
            "Epoch 135/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.3353 - accuracy: 0.8560 - val_loss: 0.9309 - val_accuracy: 0.6585\n",
            "Epoch 136/600\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.4093 - accuracy: 0.8300 - val_loss: 0.8492 - val_accuracy: 0.6768\n",
            "Epoch 137/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.2969 - accuracy: 0.8600 - val_loss: 0.8426 - val_accuracy: 0.6951\n",
            "Epoch 138/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.2606 - accuracy: 0.8920 - val_loss: 0.8925 - val_accuracy: 0.6951\n",
            "Epoch 139/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.2211 - accuracy: 0.9180 - val_loss: 0.9656 - val_accuracy: 0.6402\n",
            "Epoch 140/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.1868 - accuracy: 0.9300 - val_loss: 1.0025 - val_accuracy: 0.6463\n",
            "Epoch 141/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.1853 - accuracy: 0.9060 - val_loss: 1.0663 - val_accuracy: 0.6341\n",
            "Epoch 142/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.1588 - accuracy: 0.9380 - val_loss: 1.0753 - val_accuracy: 0.6646\n",
            "Epoch 143/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.2077 - accuracy: 0.9100 - val_loss: 1.1179 - val_accuracy: 0.6707\n",
            "Epoch 144/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.2767 - accuracy: 0.8860 - val_loss: 1.1086 - val_accuracy: 0.6524\n",
            "Epoch 145/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.2697 - accuracy: 0.8820 - val_loss: 1.0603 - val_accuracy: 0.6220\n",
            "Epoch 146/600\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.2877 - accuracy: 0.8680 - val_loss: 1.0245 - val_accuracy: 0.6280\n",
            "Epoch 147/600\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.2916 - accuracy: 0.8640 - val_loss: 0.9021 - val_accuracy: 0.6646\n",
            "Epoch 148/600\n",
            "500/500 [==============================] - 0s 532us/sample - loss: 0.1931 - accuracy: 0.9240 - val_loss: 1.0971 - val_accuracy: 0.6768\n",
            "Epoch 149/600\n",
            "500/500 [==============================] - 0s 513us/sample - loss: 0.1747 - accuracy: 0.9360 - val_loss: 1.1189 - val_accuracy: 0.6463\n",
            "Epoch 150/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.1879 - accuracy: 0.9300 - val_loss: 1.2127 - val_accuracy: 0.6524\n",
            "Epoch 151/600\n",
            "500/500 [==============================] - 0s 449us/sample - loss: 0.1838 - accuracy: 0.9240 - val_loss: 1.2317 - val_accuracy: 0.6159\n",
            "Epoch 152/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.1583 - accuracy: 0.9400 - val_loss: 1.1796 - val_accuracy: 0.6402\n",
            "Epoch 153/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.1826 - accuracy: 0.9320 - val_loss: 1.2535 - val_accuracy: 0.6098\n",
            "Epoch 154/600\n",
            "500/500 [==============================] - 0s 447us/sample - loss: 0.2140 - accuracy: 0.9040 - val_loss: 1.1885 - val_accuracy: 0.6524\n",
            "Epoch 155/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.1616 - accuracy: 0.9460 - val_loss: 1.1112 - val_accuracy: 0.6707\n",
            "Epoch 156/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.1399 - accuracy: 0.9440 - val_loss: 1.1600 - val_accuracy: 0.6524\n",
            "Epoch 157/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.1478 - accuracy: 0.9360 - val_loss: 1.3110 - val_accuracy: 0.6463\n",
            "Epoch 158/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.1220 - accuracy: 0.9580 - val_loss: 1.2027 - val_accuracy: 0.6768\n",
            "Epoch 159/600\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.1137 - accuracy: 0.9640 - val_loss: 1.2553 - val_accuracy: 0.7012\n",
            "Epoch 160/600\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.1024 - accuracy: 0.9680 - val_loss: 1.3175 - val_accuracy: 0.6524\n",
            "Epoch 161/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.0771 - accuracy: 0.9720 - val_loss: 1.3671 - val_accuracy: 0.6159\n",
            "Epoch 162/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0671 - accuracy: 0.9820 - val_loss: 1.4087 - val_accuracy: 0.6585\n",
            "Epoch 163/600\n",
            "500/500 [==============================] - 0s 452us/sample - loss: 0.0669 - accuracy: 0.9780 - val_loss: 1.4134 - val_accuracy: 0.6280\n",
            "Epoch 164/600\n",
            "500/500 [==============================] - 0s 489us/sample - loss: 0.5679 - accuracy: 0.8080 - val_loss: 1.3342 - val_accuracy: 0.5488\n",
            "Epoch 165/600\n",
            "500/500 [==============================] - 0s 497us/sample - loss: 0.6956 - accuracy: 0.7240 - val_loss: 0.7591 - val_accuracy: 0.6402\n",
            "Epoch 166/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.5434 - accuracy: 0.7460 - val_loss: 0.7559 - val_accuracy: 0.6280\n",
            "Epoch 167/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.4532 - accuracy: 0.7980 - val_loss: 0.7795 - val_accuracy: 0.6280\n",
            "Epoch 168/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.4082 - accuracy: 0.8020 - val_loss: 0.7783 - val_accuracy: 0.6646\n",
            "Epoch 169/600\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.4061 - accuracy: 0.8160 - val_loss: 0.7540 - val_accuracy: 0.6463\n",
            "Epoch 170/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.3682 - accuracy: 0.8380 - val_loss: 0.8079 - val_accuracy: 0.6402\n",
            "Epoch 171/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.3369 - accuracy: 0.8680 - val_loss: 0.8742 - val_accuracy: 0.6463\n",
            "Epoch 172/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.3746 - accuracy: 0.8280 - val_loss: 0.9864 - val_accuracy: 0.6220\n",
            "Epoch 173/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.3620 - accuracy: 0.8540 - val_loss: 0.8944 - val_accuracy: 0.6341\n",
            "Epoch 174/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.2903 - accuracy: 0.8800 - val_loss: 0.9485 - val_accuracy: 0.6646\n",
            "Epoch 175/600\n",
            "500/500 [==============================] - 0s 440us/sample - loss: 0.2683 - accuracy: 0.8980 - val_loss: 0.9745 - val_accuracy: 0.6402\n",
            "Epoch 176/600\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.5222 - accuracy: 0.7720 - val_loss: 0.8816 - val_accuracy: 0.6220\n",
            "Epoch 177/600\n",
            "500/500 [==============================] - 0s 467us/sample - loss: 0.4849 - accuracy: 0.7620 - val_loss: 0.8361 - val_accuracy: 0.6159\n",
            "Epoch 178/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.4349 - accuracy: 0.7940 - val_loss: 0.7816 - val_accuracy: 0.6341\n",
            "Epoch 179/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.3788 - accuracy: 0.8500 - val_loss: 0.7699 - val_accuracy: 0.6768\n",
            "Epoch 180/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.3511 - accuracy: 0.8560 - val_loss: 0.7674 - val_accuracy: 0.6890\n",
            "Epoch 181/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.3083 - accuracy: 0.8640 - val_loss: 0.8319 - val_accuracy: 0.6890\n",
            "Epoch 182/600\n",
            "500/500 [==============================] - 0s 464us/sample - loss: 0.2796 - accuracy: 0.8860 - val_loss: 0.8719 - val_accuracy: 0.6646\n",
            "Epoch 183/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.2453 - accuracy: 0.9120 - val_loss: 0.9271 - val_accuracy: 0.6585\n",
            "Epoch 184/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.2396 - accuracy: 0.9060 - val_loss: 0.8589 - val_accuracy: 0.6951\n",
            "Epoch 185/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.2014 - accuracy: 0.9220 - val_loss: 1.0233 - val_accuracy: 0.6646\n",
            "Epoch 186/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.1716 - accuracy: 0.9400 - val_loss: 1.0363 - val_accuracy: 0.6768\n",
            "Epoch 187/600\n",
            "500/500 [==============================] - 0s 453us/sample - loss: 0.2039 - accuracy: 0.9160 - val_loss: 1.0613 - val_accuracy: 0.6402\n",
            "Epoch 188/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.2161 - accuracy: 0.9200 - val_loss: 1.0520 - val_accuracy: 0.7134\n",
            "Epoch 189/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.1642 - accuracy: 0.9420 - val_loss: 1.0948 - val_accuracy: 0.6646\n",
            "Epoch 190/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.1403 - accuracy: 0.9580 - val_loss: 1.1381 - val_accuracy: 0.6463\n",
            "Epoch 191/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.1404 - accuracy: 0.9540 - val_loss: 1.2368 - val_accuracy: 0.6585\n",
            "Epoch 192/600\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.1361 - accuracy: 0.9540 - val_loss: 1.1905 - val_accuracy: 0.6524\n",
            "Epoch 193/600\n",
            "500/500 [==============================] - 0s 449us/sample - loss: 0.1413 - accuracy: 0.9580 - val_loss: 1.2512 - val_accuracy: 0.6341\n",
            "Epoch 194/600\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.1358 - accuracy: 0.9520 - val_loss: 1.4658 - val_accuracy: 0.6159\n",
            "Epoch 195/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.2231 - accuracy: 0.9180 - val_loss: 1.1440 - val_accuracy: 0.6829\n",
            "Epoch 196/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.2368 - accuracy: 0.9040 - val_loss: 1.2770 - val_accuracy: 0.6341\n",
            "Epoch 197/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.2174 - accuracy: 0.9120 - val_loss: 1.1193 - val_accuracy: 0.6341\n",
            "Epoch 198/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.2445 - accuracy: 0.8940 - val_loss: 1.1296 - val_accuracy: 0.6341\n",
            "Epoch 199/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.1439 - accuracy: 0.9500 - val_loss: 1.2828 - val_accuracy: 0.6524\n",
            "Epoch 200/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.1151 - accuracy: 0.9600 - val_loss: 1.4092 - val_accuracy: 0.5854\n",
            "Epoch 201/600\n",
            "500/500 [==============================] - 0s 447us/sample - loss: 0.1128 - accuracy: 0.9640 - val_loss: 1.4182 - val_accuracy: 0.6524\n",
            "Epoch 202/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.1022 - accuracy: 0.9620 - val_loss: 1.4479 - val_accuracy: 0.6220\n",
            "Epoch 203/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0806 - accuracy: 0.9760 - val_loss: 1.4405 - val_accuracy: 0.6463\n",
            "Epoch 204/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.1057 - accuracy: 0.9540 - val_loss: 1.5588 - val_accuracy: 0.6402\n",
            "Epoch 205/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.1920 - accuracy: 0.9320 - val_loss: 1.2951 - val_accuracy: 0.6402\n",
            "Epoch 206/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.1441 - accuracy: 0.9460 - val_loss: 1.3336 - val_accuracy: 0.6829\n",
            "Epoch 207/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0963 - accuracy: 0.9700 - val_loss: 1.3910 - val_accuracy: 0.6585\n",
            "Epoch 208/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.0893 - accuracy: 0.9700 - val_loss: 1.4748 - val_accuracy: 0.6402\n",
            "Epoch 209/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.1035 - accuracy: 0.9600 - val_loss: 1.4403 - val_accuracy: 0.6341\n",
            "Epoch 210/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.1047 - accuracy: 0.9560 - val_loss: 1.4096 - val_accuracy: 0.6463\n",
            "Epoch 211/600\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.0910 - accuracy: 0.9700 - val_loss: 1.4789 - val_accuracy: 0.6341\n",
            "Epoch 212/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0774 - accuracy: 0.9760 - val_loss: 1.4398 - val_accuracy: 0.6463\n",
            "Epoch 213/600\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.0617 - accuracy: 0.9760 - val_loss: 1.4648 - val_accuracy: 0.6402\n",
            "Epoch 214/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.0749 - accuracy: 0.9680 - val_loss: 1.4828 - val_accuracy: 0.6159\n",
            "Epoch 215/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0654 - accuracy: 0.9800 - val_loss: 1.5920 - val_accuracy: 0.6463\n",
            "Epoch 216/600\n",
            "500/500 [==============================] - 0s 460us/sample - loss: 0.0633 - accuracy: 0.9780 - val_loss: 1.6659 - val_accuracy: 0.6585\n",
            "Epoch 217/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.0756 - accuracy: 0.9780 - val_loss: 1.6555 - val_accuracy: 0.6402\n",
            "Epoch 218/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0795 - accuracy: 0.9740 - val_loss: 1.6347 - val_accuracy: 0.6220\n",
            "Epoch 219/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0678 - accuracy: 0.9740 - val_loss: 1.5746 - val_accuracy: 0.6402\n",
            "Epoch 220/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.2612 - accuracy: 0.9120 - val_loss: 1.4666 - val_accuracy: 0.6402\n",
            "Epoch 221/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.2140 - accuracy: 0.9200 - val_loss: 1.5110 - val_accuracy: 0.5976\n",
            "Epoch 222/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.1469 - accuracy: 0.9380 - val_loss: 1.5015 - val_accuracy: 0.5793\n",
            "Epoch 223/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.1098 - accuracy: 0.9580 - val_loss: 1.4834 - val_accuracy: 0.6220\n",
            "Epoch 224/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0827 - accuracy: 0.9820 - val_loss: 1.5028 - val_accuracy: 0.6280\n",
            "Epoch 225/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.0553 - accuracy: 0.9880 - val_loss: 1.5198 - val_accuracy: 0.6280\n",
            "Epoch 226/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.1631 - accuracy: 0.9440 - val_loss: 1.7101 - val_accuracy: 0.6098\n",
            "Epoch 227/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6212 - accuracy: 0.7860 - val_loss: 1.3747 - val_accuracy: 0.5793\n",
            "Epoch 228/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.4292 - accuracy: 0.8300 - val_loss: 1.0686 - val_accuracy: 0.6280\n",
            "Epoch 229/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.3348 - accuracy: 0.8720 - val_loss: 1.1224 - val_accuracy: 0.6220\n",
            "Epoch 230/600\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.2495 - accuracy: 0.9220 - val_loss: 1.2060 - val_accuracy: 0.5976\n",
            "Epoch 231/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.1711 - accuracy: 0.9420 - val_loss: 1.2254 - val_accuracy: 0.6037\n",
            "Epoch 232/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.1242 - accuracy: 0.9680 - val_loss: 1.2737 - val_accuracy: 0.6402\n",
            "Epoch 233/600\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.1047 - accuracy: 0.9700 - val_loss: 1.3429 - val_accuracy: 0.6280\n",
            "Epoch 234/600\n",
            "500/500 [==============================] - 0s 437us/sample - loss: 0.0931 - accuracy: 0.9760 - val_loss: 1.3683 - val_accuracy: 0.6341\n",
            "Epoch 235/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0805 - accuracy: 0.9820 - val_loss: 1.4382 - val_accuracy: 0.6280\n",
            "Epoch 236/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0714 - accuracy: 0.9800 - val_loss: 1.5036 - val_accuracy: 0.6037\n",
            "Epoch 237/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.0665 - accuracy: 0.9840 - val_loss: 1.5208 - val_accuracy: 0.6341\n",
            "Epoch 238/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.0646 - accuracy: 0.9780 - val_loss: 1.5953 - val_accuracy: 0.5976\n",
            "Epoch 239/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.0559 - accuracy: 0.9880 - val_loss: 1.5215 - val_accuracy: 0.6159\n",
            "Epoch 240/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0735 - accuracy: 0.9800 - val_loss: 1.5591 - val_accuracy: 0.6037\n",
            "Epoch 241/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0929 - accuracy: 0.9640 - val_loss: 1.4974 - val_accuracy: 0.6159\n",
            "Epoch 242/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0659 - accuracy: 0.9800 - val_loss: 1.5833 - val_accuracy: 0.6098\n",
            "Epoch 243/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0487 - accuracy: 0.9900 - val_loss: 1.6792 - val_accuracy: 0.6098\n",
            "Epoch 244/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0629 - accuracy: 0.9840 - val_loss: 1.6676 - val_accuracy: 0.6402\n",
            "Epoch 245/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.0583 - accuracy: 0.9820 - val_loss: 1.6583 - val_accuracy: 0.6098\n",
            "Epoch 246/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0364 - accuracy: 0.9900 - val_loss: 1.7086 - val_accuracy: 0.6280\n",
            "Epoch 247/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.0377 - accuracy: 0.9880 - val_loss: 1.8040 - val_accuracy: 0.6280\n",
            "Epoch 248/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0358 - accuracy: 0.9920 - val_loss: 1.7605 - val_accuracy: 0.6280\n",
            "Epoch 249/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0371 - accuracy: 0.9920 - val_loss: 1.7659 - val_accuracy: 0.6341\n",
            "Epoch 250/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.0336 - accuracy: 0.9920 - val_loss: 1.7869 - val_accuracy: 0.6159\n",
            "Epoch 251/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0272 - accuracy: 0.9940 - val_loss: 1.8760 - val_accuracy: 0.6341\n",
            "Epoch 252/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0273 - accuracy: 0.9900 - val_loss: 1.9401 - val_accuracy: 0.6159\n",
            "Epoch 253/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0241 - accuracy: 0.9940 - val_loss: 1.9227 - val_accuracy: 0.6280\n",
            "Epoch 254/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.0231 - accuracy: 0.9920 - val_loss: 1.9839 - val_accuracy: 0.6220\n",
            "Epoch 255/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0258 - accuracy: 0.9940 - val_loss: 2.0012 - val_accuracy: 0.6037\n",
            "Epoch 256/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0244 - accuracy: 0.9900 - val_loss: 1.8998 - val_accuracy: 0.6280\n",
            "Epoch 257/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0365 - accuracy: 0.9860 - val_loss: 1.9106 - val_accuracy: 0.6098\n",
            "Epoch 258/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0695 - accuracy: 0.9840 - val_loss: 1.8145 - val_accuracy: 0.6220\n",
            "Epoch 259/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.2269 - accuracy: 0.9180 - val_loss: 1.6687 - val_accuracy: 0.6220\n",
            "Epoch 260/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.2491 - accuracy: 0.9140 - val_loss: 1.6154 - val_accuracy: 0.6524\n",
            "Epoch 261/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.1658 - accuracy: 0.9380 - val_loss: 1.4925 - val_accuracy: 0.6341\n",
            "Epoch 262/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0983 - accuracy: 0.9640 - val_loss: 1.6163 - val_accuracy: 0.6159\n",
            "Epoch 263/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.1205 - accuracy: 0.9560 - val_loss: 1.5875 - val_accuracy: 0.6159\n",
            "Epoch 264/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.0696 - accuracy: 0.9800 - val_loss: 1.6117 - val_accuracy: 0.5976\n",
            "Epoch 265/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0525 - accuracy: 0.9820 - val_loss: 1.6040 - val_accuracy: 0.6280\n",
            "Epoch 266/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0467 - accuracy: 0.9840 - val_loss: 1.7609 - val_accuracy: 0.6098\n",
            "Epoch 267/600\n",
            "500/500 [==============================] - 0s 471us/sample - loss: 0.0352 - accuracy: 0.9940 - val_loss: 1.9119 - val_accuracy: 0.6098\n",
            "Epoch 268/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0314 - accuracy: 0.9880 - val_loss: 1.9624 - val_accuracy: 0.5915\n",
            "Epoch 269/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0311 - accuracy: 0.9940 - val_loss: 1.8716 - val_accuracy: 0.6098\n",
            "Epoch 270/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0312 - accuracy: 0.9900 - val_loss: 1.7804 - val_accuracy: 0.6280\n",
            "Epoch 271/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0276 - accuracy: 0.9940 - val_loss: 1.9254 - val_accuracy: 0.5976\n",
            "Epoch 272/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.5299 - accuracy: 0.8540 - val_loss: 1.4749 - val_accuracy: 0.6037\n",
            "Epoch 273/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.7394 - accuracy: 0.7200 - val_loss: 0.9796 - val_accuracy: 0.6037\n",
            "Epoch 274/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.4560 - accuracy: 0.7860 - val_loss: 0.9908 - val_accuracy: 0.6341\n",
            "Epoch 275/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.3310 - accuracy: 0.8580 - val_loss: 0.9619 - val_accuracy: 0.6829\n",
            "Epoch 276/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.2858 - accuracy: 0.8700 - val_loss: 1.1065 - val_accuracy: 0.6402\n",
            "Epoch 277/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.2700 - accuracy: 0.8880 - val_loss: 1.0661 - val_accuracy: 0.6646\n",
            "Epoch 278/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.2269 - accuracy: 0.9100 - val_loss: 1.1424 - val_accuracy: 0.6585\n",
            "Epoch 279/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.1706 - accuracy: 0.9320 - val_loss: 1.1695 - val_accuracy: 0.6524\n",
            "Epoch 280/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.1328 - accuracy: 0.9520 - val_loss: 1.2707 - val_accuracy: 0.6341\n",
            "Epoch 281/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.1282 - accuracy: 0.9560 - val_loss: 1.4539 - val_accuracy: 0.5915\n",
            "Epoch 282/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.1405 - accuracy: 0.9520 - val_loss: 1.4054 - val_accuracy: 0.6341\n",
            "Epoch 283/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.1258 - accuracy: 0.9580 - val_loss: 1.5177 - val_accuracy: 0.6098\n",
            "Epoch 284/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0846 - accuracy: 0.9780 - val_loss: 1.5175 - val_accuracy: 0.6280\n",
            "Epoch 285/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.0847 - accuracy: 0.9740 - val_loss: 1.6909 - val_accuracy: 0.5671\n",
            "Epoch 286/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0738 - accuracy: 0.9780 - val_loss: 1.6708 - val_accuracy: 0.5915\n",
            "Epoch 287/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0710 - accuracy: 0.9780 - val_loss: 1.6543 - val_accuracy: 0.5915\n",
            "Epoch 288/600\n",
            "500/500 [==============================] - 0s 453us/sample - loss: 0.1155 - accuracy: 0.9520 - val_loss: 1.6685 - val_accuracy: 0.6159\n",
            "Epoch 289/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.1383 - accuracy: 0.9460 - val_loss: 1.6658 - val_accuracy: 0.6159\n",
            "Epoch 290/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.1865 - accuracy: 0.9320 - val_loss: 1.4598 - val_accuracy: 0.6646\n",
            "Epoch 291/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.2284 - accuracy: 0.8980 - val_loss: 1.5942 - val_accuracy: 0.6220\n",
            "Epoch 292/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.2150 - accuracy: 0.9200 - val_loss: 1.5292 - val_accuracy: 0.6585\n",
            "Epoch 293/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.1303 - accuracy: 0.9600 - val_loss: 1.5486 - val_accuracy: 0.6585\n",
            "Epoch 294/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.1234 - accuracy: 0.9580 - val_loss: 1.5367 - val_accuracy: 0.6646\n",
            "Epoch 295/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.0861 - accuracy: 0.9820 - val_loss: 1.5856 - val_accuracy: 0.6341\n",
            "Epoch 296/600\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.0653 - accuracy: 0.9780 - val_loss: 1.5942 - val_accuracy: 0.6585\n",
            "Epoch 297/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0571 - accuracy: 0.9820 - val_loss: 1.6654 - val_accuracy: 0.6220\n",
            "Epoch 298/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.0751 - accuracy: 0.9820 - val_loss: 1.7078 - val_accuracy: 0.6098\n",
            "Epoch 299/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0651 - accuracy: 0.9780 - val_loss: 1.7528 - val_accuracy: 0.6159\n",
            "Epoch 300/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.0639 - accuracy: 0.9760 - val_loss: 1.5714 - val_accuracy: 0.6280\n",
            "Epoch 301/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.1251 - accuracy: 0.9500 - val_loss: 1.7038 - val_accuracy: 0.6159\n",
            "Epoch 302/600\n",
            "500/500 [==============================] - 0s 396us/sample - loss: 0.0843 - accuracy: 0.9680 - val_loss: 1.6723 - val_accuracy: 0.6463\n",
            "Epoch 303/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.0694 - accuracy: 0.9740 - val_loss: 1.7668 - val_accuracy: 0.5976\n",
            "Epoch 304/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0677 - accuracy: 0.9740 - val_loss: 1.7345 - val_accuracy: 0.6341\n",
            "Epoch 305/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0831 - accuracy: 0.9760 - val_loss: 1.7037 - val_accuracy: 0.6524\n",
            "Epoch 306/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0614 - accuracy: 0.9780 - val_loss: 1.7840 - val_accuracy: 0.6098\n",
            "Epoch 307/600\n",
            "500/500 [==============================] - 0s 449us/sample - loss: 0.0409 - accuracy: 0.9840 - val_loss: 1.6794 - val_accuracy: 0.6585\n",
            "Epoch 308/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0349 - accuracy: 0.9880 - val_loss: 1.8151 - val_accuracy: 0.6159\n",
            "Epoch 309/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0372 - accuracy: 0.9880 - val_loss: 1.8752 - val_accuracy: 0.6280\n",
            "Epoch 310/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0363 - accuracy: 0.9860 - val_loss: 1.7941 - val_accuracy: 0.6524\n",
            "Epoch 311/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0298 - accuracy: 0.9940 - val_loss: 1.9214 - val_accuracy: 0.6220\n",
            "Epoch 312/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.0407 - accuracy: 0.9880 - val_loss: 1.8931 - val_accuracy: 0.6280\n",
            "Epoch 313/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.0282 - accuracy: 0.9900 - val_loss: 1.9499 - val_accuracy: 0.6220\n",
            "Epoch 314/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.0284 - accuracy: 0.9900 - val_loss: 1.9650 - val_accuracy: 0.6402\n",
            "Epoch 315/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.0215 - accuracy: 0.9920 - val_loss: 2.0003 - val_accuracy: 0.6159\n",
            "Epoch 316/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0238 - accuracy: 0.9940 - val_loss: 1.9852 - val_accuracy: 0.6280\n",
            "Epoch 317/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0208 - accuracy: 0.9900 - val_loss: 1.9774 - val_accuracy: 0.6280\n",
            "Epoch 318/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.0180 - accuracy: 0.9920 - val_loss: 2.0474 - val_accuracy: 0.6159\n",
            "Epoch 319/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0179 - accuracy: 0.9920 - val_loss: 2.0803 - val_accuracy: 0.6037\n",
            "Epoch 320/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0205 - accuracy: 0.9940 - val_loss: 2.1070 - val_accuracy: 0.6037\n",
            "Epoch 321/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0238 - accuracy: 0.9920 - val_loss: 2.0608 - val_accuracy: 0.6280\n",
            "Epoch 322/600\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.0246 - accuracy: 0.9920 - val_loss: 2.1313 - val_accuracy: 0.6159\n",
            "Epoch 323/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.3883 - accuracy: 0.8940 - val_loss: 1.7414 - val_accuracy: 0.6220\n",
            "Epoch 324/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.6010 - accuracy: 0.8020 - val_loss: 1.5091 - val_accuracy: 0.6159\n",
            "Epoch 325/600\n",
            "500/500 [==============================] - 0s 455us/sample - loss: 0.4333 - accuracy: 0.8340 - val_loss: 1.3496 - val_accuracy: 0.6524\n",
            "Epoch 326/600\n",
            "500/500 [==============================] - 0s 441us/sample - loss: 0.2878 - accuracy: 0.8820 - val_loss: 1.1623 - val_accuracy: 0.6646\n",
            "Epoch 327/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.2049 - accuracy: 0.9220 - val_loss: 1.0957 - val_accuracy: 0.7317\n",
            "Epoch 328/600\n",
            "500/500 [==============================] - 0s 468us/sample - loss: 0.1640 - accuracy: 0.9380 - val_loss: 1.2084 - val_accuracy: 0.6829\n",
            "Epoch 329/600\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.1298 - accuracy: 0.9560 - val_loss: 1.1972 - val_accuracy: 0.6890\n",
            "Epoch 330/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.1044 - accuracy: 0.9520 - val_loss: 1.1719 - val_accuracy: 0.7073\n",
            "Epoch 331/600\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.0924 - accuracy: 0.9620 - val_loss: 1.2384 - val_accuracy: 0.6646\n",
            "Epoch 332/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.1174 - accuracy: 0.9540 - val_loss: 1.2157 - val_accuracy: 0.6890\n",
            "Epoch 333/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0803 - accuracy: 0.9720 - val_loss: 1.3603 - val_accuracy: 0.6402\n",
            "Epoch 334/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0655 - accuracy: 0.9780 - val_loss: 1.3548 - val_accuracy: 0.6585\n",
            "Epoch 335/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.0632 - accuracy: 0.9820 - val_loss: 1.4304 - val_accuracy: 0.6707\n",
            "Epoch 336/600\n",
            "500/500 [==============================] - 0s 456us/sample - loss: 0.0574 - accuracy: 0.9840 - val_loss: 1.4675 - val_accuracy: 0.6524\n",
            "Epoch 337/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0536 - accuracy: 0.9880 - val_loss: 1.4676 - val_accuracy: 0.6707\n",
            "Epoch 338/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.0769 - accuracy: 0.9800 - val_loss: 1.5472 - val_accuracy: 0.6280\n",
            "Epoch 339/600\n",
            "500/500 [==============================] - 0s 397us/sample - loss: 0.1040 - accuracy: 0.9580 - val_loss: 1.5287 - val_accuracy: 0.6220\n",
            "Epoch 340/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.1815 - accuracy: 0.9420 - val_loss: 1.3873 - val_accuracy: 0.6524\n",
            "Epoch 341/600\n",
            "500/500 [==============================] - 0s 446us/sample - loss: 0.1076 - accuracy: 0.9580 - val_loss: 1.3902 - val_accuracy: 0.6646\n",
            "Epoch 342/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.1210 - accuracy: 0.9600 - val_loss: 1.4259 - val_accuracy: 0.6280\n",
            "Epoch 343/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.1293 - accuracy: 0.9520 - val_loss: 1.4866 - val_accuracy: 0.6829\n",
            "Epoch 344/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.1162 - accuracy: 0.9520 - val_loss: 1.4526 - val_accuracy: 0.6768\n",
            "Epoch 345/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.1061 - accuracy: 0.9600 - val_loss: 1.4279 - val_accuracy: 0.6524\n",
            "Epoch 346/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0897 - accuracy: 0.9760 - val_loss: 1.4776 - val_accuracy: 0.6402\n",
            "Epoch 347/600\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.1008 - accuracy: 0.9660 - val_loss: 1.5156 - val_accuracy: 0.6890\n",
            "Epoch 348/600\n",
            "500/500 [==============================] - 0s 456us/sample - loss: 0.0591 - accuracy: 0.9860 - val_loss: 1.4209 - val_accuracy: 0.6707\n",
            "Epoch 349/600\n",
            "500/500 [==============================] - 0s 463us/sample - loss: 0.0457 - accuracy: 0.9840 - val_loss: 1.4599 - val_accuracy: 0.6890\n",
            "Epoch 350/600\n",
            "500/500 [==============================] - 0s 510us/sample - loss: 0.0355 - accuracy: 0.9920 - val_loss: 1.5003 - val_accuracy: 0.6768\n",
            "Epoch 351/600\n",
            "500/500 [==============================] - 0s 488us/sample - loss: 0.0296 - accuracy: 0.9900 - val_loss: 1.5610 - val_accuracy: 0.6646\n",
            "Epoch 352/600\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.0280 - accuracy: 0.9900 - val_loss: 1.5883 - val_accuracy: 0.6646\n",
            "Epoch 353/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.0250 - accuracy: 0.9920 - val_loss: 1.6148 - val_accuracy: 0.6341\n",
            "Epoch 354/600\n",
            "500/500 [==============================] - 0s 448us/sample - loss: 0.0240 - accuracy: 0.9900 - val_loss: 1.6377 - val_accuracy: 0.6585\n",
            "Epoch 355/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0216 - accuracy: 0.9940 - val_loss: 1.6668 - val_accuracy: 0.6341\n",
            "Epoch 356/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.0216 - accuracy: 0.9920 - val_loss: 1.6988 - val_accuracy: 0.6402\n",
            "Epoch 357/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0183 - accuracy: 0.9940 - val_loss: 1.7291 - val_accuracy: 0.6585\n",
            "Epoch 358/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0170 - accuracy: 0.9940 - val_loss: 1.7382 - val_accuracy: 0.6524\n",
            "Epoch 359/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0162 - accuracy: 0.9940 - val_loss: 1.7594 - val_accuracy: 0.6524\n",
            "Epoch 360/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0142 - accuracy: 0.9940 - val_loss: 1.7966 - val_accuracy: 0.6585\n",
            "Epoch 361/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0145 - accuracy: 0.9960 - val_loss: 1.8283 - val_accuracy: 0.6463\n",
            "Epoch 362/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0136 - accuracy: 0.9940 - val_loss: 1.8363 - val_accuracy: 0.6524\n",
            "Epoch 363/600\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 1.8652 - val_accuracy: 0.6463\n",
            "Epoch 364/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.0125 - accuracy: 0.9960 - val_loss: 1.8233 - val_accuracy: 0.6524\n",
            "Epoch 365/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0153 - accuracy: 0.9940 - val_loss: 1.8998 - val_accuracy: 0.6524\n",
            "Epoch 366/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.0138 - accuracy: 0.9960 - val_loss: 1.8176 - val_accuracy: 0.6463\n",
            "Epoch 367/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0167 - accuracy: 0.9940 - val_loss: 1.8736 - val_accuracy: 0.6524\n",
            "Epoch 368/600\n",
            "500/500 [==============================] - 0s 440us/sample - loss: 0.0163 - accuracy: 0.9940 - val_loss: 1.8063 - val_accuracy: 0.6585\n",
            "Epoch 369/600\n",
            "500/500 [==============================] - 0s 535us/sample - loss: 0.2917 - accuracy: 0.9060 - val_loss: 1.7738 - val_accuracy: 0.6463\n",
            "Epoch 370/600\n",
            "500/500 [==============================] - 0s 494us/sample - loss: 0.6680 - accuracy: 0.8140 - val_loss: 1.5995 - val_accuracy: 0.5915\n",
            "Epoch 371/600\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.4791 - accuracy: 0.8240 - val_loss: 1.1663 - val_accuracy: 0.6829\n",
            "Epoch 372/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.2668 - accuracy: 0.8840 - val_loss: 1.1435 - val_accuracy: 0.6463\n",
            "Epoch 373/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.1873 - accuracy: 0.9140 - val_loss: 1.1669 - val_accuracy: 0.6463\n",
            "Epoch 374/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.1436 - accuracy: 0.9500 - val_loss: 1.3106 - val_accuracy: 0.6098\n",
            "Epoch 375/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.1155 - accuracy: 0.9680 - val_loss: 1.2902 - val_accuracy: 0.6220\n",
            "Epoch 376/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0871 - accuracy: 0.9760 - val_loss: 1.2713 - val_accuracy: 0.6463\n",
            "Epoch 377/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.0644 - accuracy: 0.9800 - val_loss: 1.3894 - val_accuracy: 0.6159\n",
            "Epoch 378/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0529 - accuracy: 0.9820 - val_loss: 1.4447 - val_accuracy: 0.6098\n",
            "Epoch 379/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0415 - accuracy: 0.9880 - val_loss: 1.5138 - val_accuracy: 0.6341\n",
            "Epoch 380/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0343 - accuracy: 0.9900 - val_loss: 1.4993 - val_accuracy: 0.6159\n",
            "Epoch 381/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0376 - accuracy: 0.9900 - val_loss: 1.5720 - val_accuracy: 0.6341\n",
            "Epoch 382/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0359 - accuracy: 0.9920 - val_loss: 1.6001 - val_accuracy: 0.6159\n",
            "Epoch 383/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.0341 - accuracy: 0.9920 - val_loss: 1.5301 - val_accuracy: 0.6463\n",
            "Epoch 384/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0327 - accuracy: 0.9860 - val_loss: 1.5892 - val_accuracy: 0.6524\n",
            "Epoch 385/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0550 - accuracy: 0.9860 - val_loss: 1.7241 - val_accuracy: 0.6341\n",
            "Epoch 386/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.0426 - accuracy: 0.9900 - val_loss: 1.4172 - val_accuracy: 0.6585\n",
            "Epoch 387/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0866 - accuracy: 0.9760 - val_loss: 1.7036 - val_accuracy: 0.6768\n",
            "Epoch 388/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.3254 - accuracy: 0.8900 - val_loss: 1.4194 - val_accuracy: 0.6463\n",
            "Epoch 389/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.2816 - accuracy: 0.8900 - val_loss: 1.3788 - val_accuracy: 0.6585\n",
            "Epoch 390/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.2339 - accuracy: 0.9120 - val_loss: 1.3122 - val_accuracy: 0.6646\n",
            "Epoch 391/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.2217 - accuracy: 0.9080 - val_loss: 1.3628 - val_accuracy: 0.6098\n",
            "Epoch 392/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.4192 - accuracy: 0.8340 - val_loss: 1.2622 - val_accuracy: 0.6646\n",
            "Epoch 393/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.2584 - accuracy: 0.9060 - val_loss: 1.2978 - val_accuracy: 0.6707\n",
            "Epoch 394/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.2003 - accuracy: 0.9240 - val_loss: 1.1897 - val_accuracy: 0.6341\n",
            "Epoch 395/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.1268 - accuracy: 0.9520 - val_loss: 1.2054 - val_accuracy: 0.6829\n",
            "Epoch 396/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.1231 - accuracy: 0.9540 - val_loss: 1.2409 - val_accuracy: 0.6768\n",
            "Epoch 397/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.0829 - accuracy: 0.9760 - val_loss: 1.3520 - val_accuracy: 0.6768\n",
            "Epoch 398/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0992 - accuracy: 0.9640 - val_loss: 1.3645 - val_accuracy: 0.6585\n",
            "Epoch 399/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0846 - accuracy: 0.9700 - val_loss: 1.3795 - val_accuracy: 0.6707\n",
            "Epoch 400/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0707 - accuracy: 0.9780 - val_loss: 1.3809 - val_accuracy: 0.6646\n",
            "Epoch 401/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0563 - accuracy: 0.9860 - val_loss: 1.4438 - val_accuracy: 0.6524\n",
            "Epoch 402/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.0426 - accuracy: 0.9880 - val_loss: 1.4677 - val_accuracy: 0.6524\n",
            "Epoch 403/600\n",
            "500/500 [==============================] - 0s 457us/sample - loss: 0.0344 - accuracy: 0.9880 - val_loss: 1.5244 - val_accuracy: 0.6402\n",
            "Epoch 404/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0315 - accuracy: 0.9920 - val_loss: 1.5452 - val_accuracy: 0.6341\n",
            "Epoch 405/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0254 - accuracy: 0.9920 - val_loss: 1.5698 - val_accuracy: 0.6463\n",
            "Epoch 406/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0282 - accuracy: 0.9880 - val_loss: 1.6354 - val_accuracy: 0.6524\n",
            "Epoch 407/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0272 - accuracy: 0.9900 - val_loss: 1.6301 - val_accuracy: 0.6524\n",
            "Epoch 408/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0240 - accuracy: 0.9900 - val_loss: 1.6395 - val_accuracy: 0.6463\n",
            "Epoch 409/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.0212 - accuracy: 0.9920 - val_loss: 1.6851 - val_accuracy: 0.6463\n",
            "Epoch 410/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0201 - accuracy: 0.9940 - val_loss: 1.7093 - val_accuracy: 0.6524\n",
            "Epoch 411/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0207 - accuracy: 0.9960 - val_loss: 1.6860 - val_accuracy: 0.6463\n",
            "Epoch 412/600\n",
            "500/500 [==============================] - 0s 448us/sample - loss: 0.0239 - accuracy: 0.9940 - val_loss: 1.7467 - val_accuracy: 0.6402\n",
            "Epoch 413/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.0271 - accuracy: 0.9900 - val_loss: 1.7981 - val_accuracy: 0.6524\n",
            "Epoch 414/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.0210 - accuracy: 0.9900 - val_loss: 1.7455 - val_accuracy: 0.6585\n",
            "Epoch 415/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0229 - accuracy: 0.9900 - val_loss: 1.7801 - val_accuracy: 0.6585\n",
            "Epoch 416/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0772 - accuracy: 0.9780 - val_loss: 1.6542 - val_accuracy: 0.6707\n",
            "Epoch 417/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.1427 - accuracy: 0.9460 - val_loss: 1.6163 - val_accuracy: 0.6463\n",
            "Epoch 418/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0900 - accuracy: 0.9640 - val_loss: 1.5523 - val_accuracy: 0.6951\n",
            "Epoch 419/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.1049 - accuracy: 0.9660 - val_loss: 1.4829 - val_accuracy: 0.6890\n",
            "Epoch 420/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0970 - accuracy: 0.9700 - val_loss: 1.7451 - val_accuracy: 0.6341\n",
            "Epoch 421/600\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.0614 - accuracy: 0.9780 - val_loss: 1.6063 - val_accuracy: 0.6768\n",
            "Epoch 422/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0488 - accuracy: 0.9840 - val_loss: 1.6104 - val_accuracy: 0.6646\n",
            "Epoch 423/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0381 - accuracy: 0.9880 - val_loss: 1.6724 - val_accuracy: 0.6524\n",
            "Epoch 424/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0276 - accuracy: 0.9920 - val_loss: 1.7189 - val_accuracy: 0.6341\n",
            "Epoch 425/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0240 - accuracy: 0.9920 - val_loss: 1.7386 - val_accuracy: 0.6341\n",
            "Epoch 426/600\n",
            "500/500 [==============================] - 0s 442us/sample - loss: 0.0207 - accuracy: 0.9920 - val_loss: 1.7383 - val_accuracy: 0.6341\n",
            "Epoch 427/600\n",
            "500/500 [==============================] - 0s 479us/sample - loss: 0.0213 - accuracy: 0.9900 - val_loss: 1.7495 - val_accuracy: 0.6341\n",
            "Epoch 428/600\n",
            "500/500 [==============================] - 0s 491us/sample - loss: 0.0190 - accuracy: 0.9920 - val_loss: 1.7729 - val_accuracy: 0.6280\n",
            "Epoch 429/600\n",
            "500/500 [==============================] - 0s 548us/sample - loss: 0.0204 - accuracy: 0.9920 - val_loss: 1.7841 - val_accuracy: 0.6341\n",
            "Epoch 430/600\n",
            "500/500 [==============================] - 0s 464us/sample - loss: 0.0193 - accuracy: 0.9900 - val_loss: 1.7860 - val_accuracy: 0.6402\n",
            "Epoch 431/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0173 - accuracy: 0.9940 - val_loss: 1.8058 - val_accuracy: 0.6341\n",
            "Epoch 432/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.0166 - accuracy: 0.9900 - val_loss: 1.8117 - val_accuracy: 0.6341\n",
            "Epoch 433/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0145 - accuracy: 0.9940 - val_loss: 1.8284 - val_accuracy: 0.6341\n",
            "Epoch 434/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0147 - accuracy: 0.9960 - val_loss: 1.8644 - val_accuracy: 0.6159\n",
            "Epoch 435/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0160 - accuracy: 0.9960 - val_loss: 1.8698 - val_accuracy: 0.6463\n",
            "Epoch 436/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0144 - accuracy: 0.9960 - val_loss: 1.9160 - val_accuracy: 0.6280\n",
            "Epoch 437/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0145 - accuracy: 0.9960 - val_loss: 1.9154 - val_accuracy: 0.6402\n",
            "Epoch 438/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0120 - accuracy: 0.9960 - val_loss: 1.9186 - val_accuracy: 0.6402\n",
            "Epoch 439/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0118 - accuracy: 0.9940 - val_loss: 1.9243 - val_accuracy: 0.6463\n",
            "Epoch 440/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.0117 - accuracy: 0.9960 - val_loss: 1.9504 - val_accuracy: 0.6463\n",
            "Epoch 441/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0122 - accuracy: 0.9960 - val_loss: 1.9503 - val_accuracy: 0.6463\n",
            "Epoch 442/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0111 - accuracy: 0.9960 - val_loss: 1.9668 - val_accuracy: 0.6463\n",
            "Epoch 443/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0114 - accuracy: 0.9960 - val_loss: 1.9678 - val_accuracy: 0.6341\n",
            "Epoch 444/600\n",
            "500/500 [==============================] - 0s 479us/sample - loss: 0.0105 - accuracy: 0.9960 - val_loss: 1.9631 - val_accuracy: 0.6341\n",
            "Epoch 445/600\n",
            "500/500 [==============================] - 0s 535us/sample - loss: 0.0101 - accuracy: 0.9960 - val_loss: 1.9868 - val_accuracy: 0.6524\n",
            "Epoch 446/600\n",
            "500/500 [==============================] - 0s 476us/sample - loss: 0.0113 - accuracy: 0.9920 - val_loss: 2.0257 - val_accuracy: 0.6463\n",
            "Epoch 447/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0122 - accuracy: 0.9940 - val_loss: 2.0418 - val_accuracy: 0.6341\n",
            "Epoch 448/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.0156 - accuracy: 0.9940 - val_loss: 2.0106 - val_accuracy: 0.6463\n",
            "Epoch 449/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0230 - accuracy: 0.9900 - val_loss: 1.8713 - val_accuracy: 0.6524\n",
            "Epoch 450/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0277 - accuracy: 0.9920 - val_loss: 1.9742 - val_accuracy: 0.6646\n",
            "Epoch 451/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.0247 - accuracy: 0.9940 - val_loss: 2.0759 - val_accuracy: 0.6768\n",
            "Epoch 452/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0312 - accuracy: 0.9900 - val_loss: 1.8709 - val_accuracy: 0.6707\n",
            "Epoch 453/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0391 - accuracy: 0.9900 - val_loss: 1.8963 - val_accuracy: 0.6463\n",
            "Epoch 454/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0454 - accuracy: 0.9820 - val_loss: 2.0212 - val_accuracy: 0.6280\n",
            "Epoch 455/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0281 - accuracy: 0.9880 - val_loss: 2.0001 - val_accuracy: 0.6463\n",
            "Epoch 456/600\n",
            "500/500 [==============================] - 0s 399us/sample - loss: 0.0264 - accuracy: 0.9900 - val_loss: 1.9971 - val_accuracy: 0.6159\n",
            "Epoch 457/600\n",
            "500/500 [==============================] - 0s 396us/sample - loss: 0.0228 - accuracy: 0.9920 - val_loss: 1.9585 - val_accuracy: 0.6098\n",
            "Epoch 458/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0275 - accuracy: 0.9880 - val_loss: 1.9839 - val_accuracy: 0.6220\n",
            "Epoch 459/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0177 - accuracy: 0.9940 - val_loss: 1.9590 - val_accuracy: 0.6341\n",
            "Epoch 460/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0161 - accuracy: 0.9940 - val_loss: 1.9970 - val_accuracy: 0.6159\n",
            "Epoch 461/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0140 - accuracy: 0.9940 - val_loss: 2.0058 - val_accuracy: 0.6341\n",
            "Epoch 462/600\n",
            "500/500 [==============================] - 0s 400us/sample - loss: 0.0119 - accuracy: 0.9960 - val_loss: 2.0292 - val_accuracy: 0.6220\n",
            "Epoch 463/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.0120 - accuracy: 0.9960 - val_loss: 2.0567 - val_accuracy: 0.6280\n",
            "Epoch 464/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.0124 - accuracy: 0.9940 - val_loss: 2.0946 - val_accuracy: 0.6280\n",
            "Epoch 465/600\n",
            "500/500 [==============================] - 0s 413us/sample - loss: 0.0161 - accuracy: 0.9920 - val_loss: 2.0787 - val_accuracy: 0.6463\n",
            "Epoch 466/600\n",
            "500/500 [==============================] - 0s 425us/sample - loss: 0.0122 - accuracy: 0.9960 - val_loss: 2.0669 - val_accuracy: 0.6402\n",
            "Epoch 467/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0136 - accuracy: 0.9940 - val_loss: 2.0972 - val_accuracy: 0.6463\n",
            "Epoch 468/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.0150 - accuracy: 0.9920 - val_loss: 2.1093 - val_accuracy: 0.6463\n",
            "Epoch 469/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0121 - accuracy: 0.9940 - val_loss: 2.1199 - val_accuracy: 0.6463\n",
            "Epoch 470/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0116 - accuracy: 0.9940 - val_loss: 2.1155 - val_accuracy: 0.6280\n",
            "Epoch 471/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0104 - accuracy: 0.9960 - val_loss: 2.0770 - val_accuracy: 0.6524\n",
            "Epoch 472/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0100 - accuracy: 0.9960 - val_loss: 2.0949 - val_accuracy: 0.6585\n",
            "Epoch 473/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0097 - accuracy: 0.9960 - val_loss: 2.1340 - val_accuracy: 0.6524\n",
            "Epoch 474/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.0098 - accuracy: 0.9960 - val_loss: 2.1376 - val_accuracy: 0.6463\n",
            "Epoch 475/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0092 - accuracy: 0.9960 - val_loss: 2.1211 - val_accuracy: 0.6524\n",
            "Epoch 476/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.0094 - accuracy: 0.9960 - val_loss: 2.1519 - val_accuracy: 0.6463\n",
            "Epoch 477/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0099 - accuracy: 0.9960 - val_loss: 2.1554 - val_accuracy: 0.6463\n",
            "Epoch 478/600\n",
            "500/500 [==============================] - 0s 440us/sample - loss: 0.0109 - accuracy: 0.9940 - val_loss: 2.1286 - val_accuracy: 0.6463\n",
            "Epoch 479/600\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.0103 - accuracy: 0.9960 - val_loss: 2.1585 - val_accuracy: 0.6341\n",
            "Epoch 480/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0110 - accuracy: 0.9900 - val_loss: 2.1626 - val_accuracy: 0.6463\n",
            "Epoch 481/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0099 - accuracy: 0.9960 - val_loss: 2.1737 - val_accuracy: 0.6402\n",
            "Epoch 482/600\n",
            "500/500 [==============================] - 0s 470us/sample - loss: 0.0092 - accuracy: 0.9960 - val_loss: 2.1610 - val_accuracy: 0.6463\n",
            "Epoch 483/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0085 - accuracy: 0.9960 - val_loss: 2.1667 - val_accuracy: 0.6463\n",
            "Epoch 484/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.1734 - val_accuracy: 0.6463\n",
            "Epoch 485/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0086 - accuracy: 0.9960 - val_loss: 2.1801 - val_accuracy: 0.6463\n",
            "Epoch 486/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0094 - accuracy: 0.9940 - val_loss: 2.1867 - val_accuracy: 0.6402\n",
            "Epoch 487/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.1869 - val_accuracy: 0.6463\n",
            "Epoch 488/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0088 - accuracy: 0.9960 - val_loss: 2.1916 - val_accuracy: 0.6463\n",
            "Epoch 489/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0085 - accuracy: 0.9960 - val_loss: 2.2004 - val_accuracy: 0.6402\n",
            "Epoch 490/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0090 - accuracy: 0.9960 - val_loss: 2.2032 - val_accuracy: 0.6402\n",
            "Epoch 491/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0084 - accuracy: 0.9960 - val_loss: 2.2023 - val_accuracy: 0.6402\n",
            "Epoch 492/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.0084 - accuracy: 0.9940 - val_loss: 2.1991 - val_accuracy: 0.6402\n",
            "Epoch 493/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.2061 - val_accuracy: 0.6463\n",
            "Epoch 494/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0080 - accuracy: 0.9940 - val_loss: 2.2043 - val_accuracy: 0.6402\n",
            "Epoch 495/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.0089 - accuracy: 0.9960 - val_loss: 2.2159 - val_accuracy: 0.6524\n",
            "Epoch 496/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.0091 - accuracy: 0.9960 - val_loss: 2.2173 - val_accuracy: 0.6463\n",
            "Epoch 497/600\n",
            "500/500 [==============================] - 0s 453us/sample - loss: 0.0099 - accuracy: 0.9960 - val_loss: 2.2291 - val_accuracy: 0.6463\n",
            "Epoch 498/600\n",
            "500/500 [==============================] - 0s 454us/sample - loss: 0.0098 - accuracy: 0.9940 - val_loss: 2.2117 - val_accuracy: 0.6463\n",
            "Epoch 499/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0085 - accuracy: 0.9960 - val_loss: 2.2220 - val_accuracy: 0.6402\n",
            "Epoch 500/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0080 - accuracy: 0.9960 - val_loss: 2.2288 - val_accuracy: 0.6463\n",
            "Epoch 501/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0100 - accuracy: 0.9900 - val_loss: 2.2328 - val_accuracy: 0.6463\n",
            "Epoch 502/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0081 - accuracy: 0.9960 - val_loss: 2.2142 - val_accuracy: 0.6280\n",
            "Epoch 503/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.0087 - accuracy: 0.9940 - val_loss: 2.2177 - val_accuracy: 0.6341\n",
            "Epoch 504/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.0091 - accuracy: 0.9960 - val_loss: 2.2294 - val_accuracy: 0.6402\n",
            "Epoch 505/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0084 - accuracy: 0.9940 - val_loss: 2.2438 - val_accuracy: 0.6341\n",
            "Epoch 506/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.0091 - accuracy: 0.9960 - val_loss: 2.2270 - val_accuracy: 0.6280\n",
            "Epoch 507/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.0080 - accuracy: 0.9960 - val_loss: 2.2298 - val_accuracy: 0.6341\n",
            "Epoch 508/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0076 - accuracy: 0.9960 - val_loss: 2.2391 - val_accuracy: 0.6402\n",
            "Epoch 509/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0085 - accuracy: 0.9960 - val_loss: 2.2408 - val_accuracy: 0.6402\n",
            "Epoch 510/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0105 - accuracy: 0.9900 - val_loss: 2.2512 - val_accuracy: 0.6463\n",
            "Epoch 511/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0077 - accuracy: 0.9960 - val_loss: 2.2395 - val_accuracy: 0.6280\n",
            "Epoch 512/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.0084 - accuracy: 0.9960 - val_loss: 2.2402 - val_accuracy: 0.6341\n",
            "Epoch 513/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.2422 - val_accuracy: 0.6341\n",
            "Epoch 514/600\n",
            "500/500 [==============================] - 0s 436us/sample - loss: 0.0094 - accuracy: 0.9960 - val_loss: 2.2378 - val_accuracy: 0.6341\n",
            "Epoch 515/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.0091 - accuracy: 0.9960 - val_loss: 2.2446 - val_accuracy: 0.6341\n",
            "Epoch 516/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.0090 - accuracy: 0.9960 - val_loss: 2.2542 - val_accuracy: 0.6341\n",
            "Epoch 517/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0089 - accuracy: 0.9940 - val_loss: 2.2431 - val_accuracy: 0.6280\n",
            "Epoch 518/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0083 - accuracy: 0.9960 - val_loss: 2.2423 - val_accuracy: 0.6341\n",
            "Epoch 519/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0079 - accuracy: 0.9960 - val_loss: 2.2540 - val_accuracy: 0.6402\n",
            "Epoch 520/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0081 - accuracy: 0.9960 - val_loss: 2.2553 - val_accuracy: 0.6402\n",
            "Epoch 521/600\n",
            "500/500 [==============================] - 0s 405us/sample - loss: 0.0094 - accuracy: 0.9940 - val_loss: 2.2517 - val_accuracy: 0.6341\n",
            "Epoch 522/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0087 - accuracy: 0.9940 - val_loss: 2.2538 - val_accuracy: 0.6402\n",
            "Epoch 523/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.0092 - accuracy: 0.9960 - val_loss: 2.2624 - val_accuracy: 0.6341\n",
            "Epoch 524/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.0086 - accuracy: 0.9940 - val_loss: 2.2615 - val_accuracy: 0.6463\n",
            "Epoch 525/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.2597 - val_accuracy: 0.6463\n",
            "Epoch 526/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.0092 - accuracy: 0.9900 - val_loss: 2.2560 - val_accuracy: 0.6524\n",
            "Epoch 527/600\n",
            "500/500 [==============================] - 0s 429us/sample - loss: 0.0081 - accuracy: 0.9960 - val_loss: 2.2606 - val_accuracy: 0.6402\n",
            "Epoch 528/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0078 - accuracy: 0.9960 - val_loss: 2.2675 - val_accuracy: 0.6402\n",
            "Epoch 529/600\n",
            "500/500 [==============================] - 0s 443us/sample - loss: 0.0079 - accuracy: 0.9940 - val_loss: 2.2716 - val_accuracy: 0.6402\n",
            "Epoch 530/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.0083 - accuracy: 0.9940 - val_loss: 2.2726 - val_accuracy: 0.6402\n",
            "Epoch 531/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.0077 - accuracy: 0.9960 - val_loss: 2.2751 - val_accuracy: 0.6463\n",
            "Epoch 532/600\n",
            "500/500 [==============================] - 0s 452us/sample - loss: 0.0081 - accuracy: 0.9940 - val_loss: 2.2737 - val_accuracy: 0.6463\n",
            "Epoch 533/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.0086 - accuracy: 0.9960 - val_loss: 2.2902 - val_accuracy: 0.6341\n",
            "Epoch 534/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.0083 - accuracy: 0.9940 - val_loss: 2.2804 - val_accuracy: 0.6463\n",
            "Epoch 535/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.0081 - accuracy: 0.9960 - val_loss: 2.2777 - val_accuracy: 0.6463\n",
            "Epoch 536/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0076 - accuracy: 0.9940 - val_loss: 2.2885 - val_accuracy: 0.6524\n",
            "Epoch 537/600\n",
            "500/500 [==============================] - 0s 445us/sample - loss: 0.0082 - accuracy: 0.9960 - val_loss: 2.2924 - val_accuracy: 0.6463\n",
            "Epoch 538/600\n",
            "500/500 [==============================] - 0s 415us/sample - loss: 0.0085 - accuracy: 0.9940 - val_loss: 2.2918 - val_accuracy: 0.6463\n",
            "Epoch 539/600\n",
            "500/500 [==============================] - 0s 431us/sample - loss: 0.0076 - accuracy: 0.9940 - val_loss: 2.3043 - val_accuracy: 0.6402\n",
            "Epoch 540/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.0078 - accuracy: 0.9960 - val_loss: 2.3113 - val_accuracy: 0.6402\n",
            "Epoch 541/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.0074 - accuracy: 0.9960 - val_loss: 2.3013 - val_accuracy: 0.6463\n",
            "Epoch 542/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.0078 - accuracy: 0.9960 - val_loss: 2.3027 - val_accuracy: 0.6463\n",
            "Epoch 543/600\n",
            "500/500 [==============================] - 0s 422us/sample - loss: 0.0072 - accuracy: 0.9960 - val_loss: 2.3137 - val_accuracy: 0.6463\n",
            "Epoch 544/600\n",
            "500/500 [==============================] - 0s 439us/sample - loss: 0.0084 - accuracy: 0.9940 - val_loss: 2.3069 - val_accuracy: 0.6463\n",
            "Epoch 545/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0087 - accuracy: 0.9960 - val_loss: 2.3394 - val_accuracy: 0.6341\n",
            "Epoch 546/600\n",
            "500/500 [==============================] - 0s 430us/sample - loss: 0.0079 - accuracy: 0.9960 - val_loss: 2.3132 - val_accuracy: 0.6463\n",
            "Epoch 547/600\n",
            "500/500 [==============================] - 0s 459us/sample - loss: 0.0082 - accuracy: 0.9960 - val_loss: 2.3116 - val_accuracy: 0.6524\n",
            "Epoch 548/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.0077 - accuracy: 0.9940 - val_loss: 2.3216 - val_accuracy: 0.6463\n",
            "Epoch 549/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.0085 - accuracy: 0.9960 - val_loss: 2.3193 - val_accuracy: 0.6524\n",
            "Epoch 550/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.0088 - accuracy: 0.9940 - val_loss: 2.3124 - val_accuracy: 0.6524\n",
            "Epoch 551/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.0073 - accuracy: 0.9940 - val_loss: 2.3534 - val_accuracy: 0.6402\n",
            "Epoch 552/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0083 - accuracy: 0.9940 - val_loss: 2.3504 - val_accuracy: 0.6402\n",
            "Epoch 553/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.0437 - accuracy: 0.9800 - val_loss: 2.4072 - val_accuracy: 0.6098\n",
            "Epoch 554/600\n",
            "500/500 [==============================] - 0s 416us/sample - loss: 0.3836 - accuracy: 0.8840 - val_loss: 2.0264 - val_accuracy: 0.6524\n",
            "Epoch 555/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.7640 - accuracy: 0.7580 - val_loss: 1.5697 - val_accuracy: 0.6159\n",
            "Epoch 556/600\n",
            "500/500 [==============================] - 0s 433us/sample - loss: 0.6320 - accuracy: 0.7500 - val_loss: 1.1255 - val_accuracy: 0.6098\n",
            "Epoch 557/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.4392 - accuracy: 0.8080 - val_loss: 1.0010 - val_accuracy: 0.6463\n",
            "Epoch 558/600\n",
            "500/500 [==============================] - 0s 410us/sample - loss: 0.3799 - accuracy: 0.8340 - val_loss: 0.9703 - val_accuracy: 0.6402\n",
            "Epoch 559/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.3212 - accuracy: 0.8600 - val_loss: 1.1103 - val_accuracy: 0.6402\n",
            "Epoch 560/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.2705 - accuracy: 0.9060 - val_loss: 1.1354 - val_accuracy: 0.6646\n",
            "Epoch 561/600\n",
            "500/500 [==============================] - 0s 420us/sample - loss: 0.2128 - accuracy: 0.9140 - val_loss: 1.1348 - val_accuracy: 0.6768\n",
            "Epoch 562/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.2078 - accuracy: 0.9280 - val_loss: 1.2105 - val_accuracy: 0.6707\n",
            "Epoch 563/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.1576 - accuracy: 0.9420 - val_loss: 1.2230 - val_accuracy: 0.6829\n",
            "Epoch 564/600\n",
            "500/500 [==============================] - 0s 418us/sample - loss: 0.1314 - accuracy: 0.9540 - val_loss: 1.3041 - val_accuracy: 0.6951\n",
            "Epoch 565/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.1100 - accuracy: 0.9700 - val_loss: 1.3772 - val_accuracy: 0.6768\n",
            "Epoch 566/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.1054 - accuracy: 0.9640 - val_loss: 1.3214 - val_accuracy: 0.6707\n",
            "Epoch 567/600\n",
            "500/500 [==============================] - 0s 406us/sample - loss: 0.0943 - accuracy: 0.9700 - val_loss: 1.3810 - val_accuracy: 0.6951\n",
            "Epoch 568/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.0695 - accuracy: 0.9800 - val_loss: 1.5390 - val_accuracy: 0.6524\n",
            "Epoch 569/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.5148 - accuracy: 0.8180 - val_loss: 1.4707 - val_accuracy: 0.6280\n",
            "Epoch 570/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.8346 - accuracy: 0.6800 - val_loss: 1.0790 - val_accuracy: 0.5732\n",
            "Epoch 571/600\n",
            "500/500 [==============================] - 0s 453us/sample - loss: 0.5480 - accuracy: 0.7720 - val_loss: 0.8949 - val_accuracy: 0.6524\n",
            "Epoch 572/600\n",
            "500/500 [==============================] - 0s 424us/sample - loss: 0.3954 - accuracy: 0.8340 - val_loss: 0.9809 - val_accuracy: 0.6341\n",
            "Epoch 573/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.3140 - accuracy: 0.8680 - val_loss: 0.9721 - val_accuracy: 0.6280\n",
            "Epoch 574/600\n",
            "500/500 [==============================] - 0s 407us/sample - loss: 0.2579 - accuracy: 0.8960 - val_loss: 0.9735 - val_accuracy: 0.6463\n",
            "Epoch 575/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.2340 - accuracy: 0.9080 - val_loss: 1.0054 - val_accuracy: 0.6707\n",
            "Epoch 576/600\n",
            "500/500 [==============================] - 0s 408us/sample - loss: 0.1980 - accuracy: 0.9140 - val_loss: 1.0570 - val_accuracy: 0.6524\n",
            "Epoch 577/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.2369 - accuracy: 0.9120 - val_loss: 1.0850 - val_accuracy: 0.6707\n",
            "Epoch 578/600\n",
            "500/500 [==============================] - 0s 421us/sample - loss: 0.2203 - accuracy: 0.9060 - val_loss: 1.1845 - val_accuracy: 0.6341\n",
            "Epoch 579/600\n",
            "500/500 [==============================] - 0s 438us/sample - loss: 0.1776 - accuracy: 0.9300 - val_loss: 1.1662 - val_accuracy: 0.6707\n",
            "Epoch 580/600\n",
            "500/500 [==============================] - 0s 435us/sample - loss: 0.1604 - accuracy: 0.9320 - val_loss: 1.1676 - val_accuracy: 0.6707\n",
            "Epoch 581/600\n",
            "500/500 [==============================] - 0s 427us/sample - loss: 0.1434 - accuracy: 0.9420 - val_loss: 1.1242 - val_accuracy: 0.6890\n",
            "Epoch 582/600\n",
            "500/500 [==============================] - 0s 432us/sample - loss: 0.0998 - accuracy: 0.9680 - val_loss: 1.2716 - val_accuracy: 0.6646\n",
            "Epoch 583/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.0906 - accuracy: 0.9700 - val_loss: 1.2432 - val_accuracy: 0.6585\n",
            "Epoch 584/600\n",
            "500/500 [==============================] - 0s 411us/sample - loss: 0.0836 - accuracy: 0.9680 - val_loss: 1.2617 - val_accuracy: 0.6707\n",
            "Epoch 585/600\n",
            "500/500 [==============================] - 0s 417us/sample - loss: 0.0732 - accuracy: 0.9800 - val_loss: 1.2987 - val_accuracy: 0.6585\n",
            "Epoch 586/600\n",
            "500/500 [==============================] - 0s 403us/sample - loss: 0.4612 - accuracy: 0.8500 - val_loss: 1.2843 - val_accuracy: 0.6037\n",
            "Epoch 587/600\n",
            "500/500 [==============================] - 0s 401us/sample - loss: 0.7986 - accuracy: 0.6840 - val_loss: 0.9642 - val_accuracy: 0.6098\n",
            "Epoch 588/600\n",
            "500/500 [==============================] - 0s 409us/sample - loss: 0.5885 - accuracy: 0.7360 - val_loss: 0.8472 - val_accuracy: 0.6585\n",
            "Epoch 589/600\n",
            "500/500 [==============================] - 0s 434us/sample - loss: 0.4470 - accuracy: 0.7940 - val_loss: 0.7862 - val_accuracy: 0.6585\n",
            "Epoch 590/600\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.3924 - accuracy: 0.8260 - val_loss: 0.7930 - val_accuracy: 0.6768\n",
            "Epoch 591/600\n",
            "500/500 [==============================] - 0s 444us/sample - loss: 0.3596 - accuracy: 0.8560 - val_loss: 0.8152 - val_accuracy: 0.6951\n",
            "Epoch 592/600\n",
            "500/500 [==============================] - 0s 412us/sample - loss: 0.3262 - accuracy: 0.8700 - val_loss: 0.8450 - val_accuracy: 0.6646\n",
            "Epoch 593/600\n",
            "500/500 [==============================] - 0s 426us/sample - loss: 0.2957 - accuracy: 0.8740 - val_loss: 0.8650 - val_accuracy: 0.6585\n",
            "Epoch 594/600\n",
            "500/500 [==============================] - 0s 423us/sample - loss: 0.2734 - accuracy: 0.8860 - val_loss: 0.9171 - val_accuracy: 0.6524\n",
            "Epoch 595/600\n",
            "500/500 [==============================] - 0s 414us/sample - loss: 0.3527 - accuracy: 0.8380 - val_loss: 0.9830 - val_accuracy: 0.5976\n",
            "Epoch 596/600\n",
            "500/500 [==============================] - 0s 402us/sample - loss: 0.6816 - accuracy: 0.6560 - val_loss: 0.8070 - val_accuracy: 0.6463\n",
            "Epoch 597/600\n",
            "500/500 [==============================] - 0s 404us/sample - loss: 0.5082 - accuracy: 0.7500 - val_loss: 0.7478 - val_accuracy: 0.6646\n",
            "Epoch 598/600\n",
            "500/500 [==============================] - 0s 419us/sample - loss: 0.4506 - accuracy: 0.7880 - val_loss: 0.7523 - val_accuracy: 0.6341\n",
            "Epoch 599/600\n",
            "500/500 [==============================] - 0s 450us/sample - loss: 0.4125 - accuracy: 0.8020 - val_loss: 0.7610 - val_accuracy: 0.6341\n",
            "Epoch 600/600\n",
            "500/500 [==============================] - 0s 428us/sample - loss: 0.3774 - accuracy: 0.8280 - val_loss: 0.7674 - val_accuracy: 0.6280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iJ__v9LVAVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}